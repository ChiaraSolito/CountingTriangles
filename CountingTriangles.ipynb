{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SetUp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "from os.path import abspath\n",
    "os.environ['SPARK_HOME'] = '/home/chiara/Documenti/BigData/CountingTriangles/spark-3.5.0-bin-hadoop3'\n",
    "os.environ['HADOOP_HOME'] = '/home/chiara/Documenti/BigData/CountingTriangles/spark-3.5.0-bin-hadoop3'\n",
    "os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-17-openjdk-amd64'\n",
    "os.environ['SPARK_LOCAL_IP'] = '172.17.0.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_python = os.path.join(os.environ.get('SPARK_HOME',None),'python')\n",
    "py4j = glob.glob(os.path.join(spark_python,'lib','py4j-*.zip'))[0]\n",
    "graphf = glob.glob(os.path.join(spark_python,'graphframes.zip'))[0]\n",
    "sparkmeasure = glob.glob(os.path.join(spark_python,'sparkmeasure.zip'))[0]\n",
    "sys.path[:0]=[spark_python,py4j]\n",
    "sys.path[:0]=[spark_python,graphf]\n",
    "sys.path[:0]=[spark_python, sparkmeasure]\n",
    "os.environ['PYTHONPATH']=py4j+os.pathsep+graphf+sparkmeasure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/chiara/Documenti/BigData/CountingTriangles/spark-3.5.0-bin-hadoop3'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Counting Triangles\").enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphframes import *\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset/Wiki-Vote.txt','r') as f:\n",
    "    content = f.readlines()\n",
    "\n",
    "edges_list = list(filter( lambda x: not x.startswith('#') ,content))\n",
    "edges = list(map(lambda x: tuple(x.split('\\t')), edges_list))\n",
    "edges_tuples = list(map(lambda x: (int(x[0]), int(x[1].replace('\\n',''))), edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType, Row\n",
    "\n",
    "list1, list2 = zip(*edges_tuples)\n",
    "nodes = list(set(list1 + list2))\n",
    "nodes_tuple = [Row(x) for x in nodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, collect_list\n",
    "\n",
    "#get list of nodes, with columns renamed value and id\n",
    "vertices = spark.createDataFrame(nodes, IntegerType())\n",
    "vertices = vertices.withColumnRenamed('value','id')\n",
    "\n",
    "#get edges such that the src node is always smaller then the dst node\n",
    "edges_n = spark.createDataFrame(edges_tuples,[\"src\", \"dst\"],IntegerType())\n",
    "edges_inverted = edges_n.filter(edges_n.src > edges_n.dst)\n",
    "edges_normal = edges_n.filter(edges_n.src < edges_n.dst)\n",
    "edges_normal2 = edges_inverted.select(col('dst').alias('src'),col('src').alias('dst'))\n",
    "edges = edges_normal.union(edges_normal2).distinct()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "e1 = edges.alias('e1')\n",
    "e2 = edges.alias('e2')\n",
    "e3 = edges.alias('e3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 45:===========================>                           (12 + 12) / 24]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|src| dst|\n",
      "+---+----+\n",
      "|  3| 586|\n",
      "| 25| 255|\n",
      "| 25| 590|\n",
      "|  7|1193|\n",
      "|  8| 232|\n",
      "|  8| 607|\n",
      "| 11| 958|\n",
      "| 11|1437|\n",
      "| 11|2595|\n",
      "| 19|  61|\n",
      "| 23| 302|\n",
      "| 47|3352|\n",
      "| 29|3717|\n",
      "| 29|4088|\n",
      "| 86|1305|\n",
      "| 99|5812|\n",
      "|103|2871|\n",
      "|108|8283|\n",
      "|109|7052|\n",
      "|122|3258|\n",
      "+---+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "e1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparkmeasure import StageMetrics\n",
    "\n",
    "stagemetrics = StageMetrics(spark)\n",
    "stagemetrics.begin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "result = e1.join(e2, col(\"e1.src\") == col(\"e2.src\")) \\\n",
    "    .join(e3, (col(\"e1.dst\") == col(\"e3.src\")) & (col(\"e2.dst\") == col(\"e3.dst\"))) \\\n",
    "    .select(col(\"e1.src\").alias(\"node1\"), col(\"e1.dst\").alias(\"node2\"), col(\"e2.dst\").alias(\"node3\")).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "608389"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "stagemetrics.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[node1#289L, node2#290L, node3#291L], functions=[])\n",
      "   +- HashAggregate(keys=[node1#289L, node2#290L, node3#291L], functions=[])\n",
      "      +- Project [src#240L AS node1#289L, dst#241L AS node2#290L, dst#262L AS node3#291L]\n",
      "         +- SortMergeJoin [dst#241L, dst#262L], [src#273L, dst#274L], Inner\n",
      "            :- Sort [dst#241L ASC NULLS FIRST, dst#262L ASC NULLS FIRST], false, 0\n",
      "            :  +- Exchange hashpartitioning(dst#241L, dst#262L, 200), ENSURE_REQUIREMENTS, [plan_id=5347]\n",
      "            :     +- Project [src#240L, dst#241L, dst#262L]\n",
      "            :        +- SortMergeJoin [src#240L], [src#261L], Inner\n",
      "            :           :- Sort [src#240L ASC NULLS FIRST], false, 0\n",
      "            :           :  +- Exchange hashpartitioning(src#240L, 200), ENSURE_REQUIREMENTS, [plan_id=5337]\n",
      "            :           :     +- HashAggregate(keys=[src#240L, dst#241L], functions=[])\n",
      "            :           :        +- Exchange hashpartitioning(src#240L, dst#241L, 200), ENSURE_REQUIREMENTS, [plan_id=5331]\n",
      "            :           :           +- HashAggregate(keys=[src#240L, dst#241L], functions=[])\n",
      "            :           :              +- Union\n",
      "            :           :                 :- Filter ((isnotnull(src#240L) AND isnotnull(dst#241L)) AND (src#240L < dst#241L))\n",
      "            :           :                 :  +- Scan ExistingRDD[src#240L,dst#241L]\n",
      "            :           :                 +- Project [dst#249L AS src#244L, src#248L AS dst#245L]\n",
      "            :           :                    +- Filter ((isnotnull(src#248L) AND isnotnull(dst#249L)) AND (src#248L > dst#249L))\n",
      "            :           :                       +- Scan ExistingRDD[src#248L,dst#249L]\n",
      "            :           +- Sort [src#261L ASC NULLS FIRST], false, 0\n",
      "            :              +- Exchange hashpartitioning(src#261L, 200), ENSURE_REQUIREMENTS, [plan_id=5338]\n",
      "            :                 +- HashAggregate(keys=[src#261L, dst#262L], functions=[])\n",
      "            :                    +- Exchange hashpartitioning(src#261L, dst#262L, 200), ENSURE_REQUIREMENTS, [plan_id=5333]\n",
      "            :                       +- HashAggregate(keys=[src#261L, dst#262L], functions=[])\n",
      "            :                          +- Union\n",
      "            :                             :- Filter ((isnotnull(src#261L) AND isnotnull(dst#262L)) AND (src#261L < dst#262L))\n",
      "            :                             :  +- Scan ExistingRDD[src#261L,dst#262L]\n",
      "            :                             +- Project [dst#264L AS src#244L, src#263L AS dst#245L]\n",
      "            :                                +- Filter ((isnotnull(src#263L) AND isnotnull(dst#264L)) AND (src#263L > dst#264L))\n",
      "            :                                   +- Scan ExistingRDD[src#263L,dst#264L]\n",
      "            +- Sort [src#273L ASC NULLS FIRST, dst#274L ASC NULLS FIRST], false, 0\n",
      "               +- HashAggregate(keys=[src#273L, dst#274L], functions=[])\n",
      "                  +- Exchange hashpartitioning(src#273L, dst#274L, 200), ENSURE_REQUIREMENTS, [plan_id=5343]\n",
      "                     +- HashAggregate(keys=[src#273L, dst#274L], functions=[])\n",
      "                        +- Union\n",
      "                           :- Filter ((isnotnull(src#273L) AND isnotnull(dst#274L)) AND (src#273L < dst#274L))\n",
      "                           :  +- Scan ExistingRDD[src#273L,dst#274L]\n",
      "                           +- Project [dst#276L AS src#244L, src#275L AS dst#245L]\n",
      "                              +- Filter ((isnotnull(src#275L) AND isnotnull(dst#276L)) AND (src#275L > dst#276L))\n",
      "                                 +- Scan ExistingRDD[src#275L,dst#276L]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure the performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scheduling mode = FIFO\n",
      "Spark Context default degree of parallelism = 12\n",
      "\n",
      "Aggregated Spark stage metrics:\n",
      "numStages => 5\n",
      "numTasks => 28\n",
      "elapsedTime => 2995 (3 s)\n",
      "stageDuration => 2965 (3 s)\n",
      "executorRunTime => 10837 (11 s)\n",
      "executorCpuTime => 3348 (3 s)\n",
      "executorDeserializeTime => 80 (80 ms)\n",
      "executorDeserializeCpuTime => 73 (73 ms)\n",
      "resultSerializationTime => 0 (0 ms)\n",
      "jvmGCTime => 778 (0,8 s)\n",
      "shuffleFetchWaitTime => 0 (0 ms)\n",
      "shuffleWriteTime => 1663 (2 s)\n",
      "resultSize => 821221 (802,0 KB)\n",
      "diskBytesSpilled => 0 (0 Bytes)\n",
      "memoryBytesSpilled => 0 (0 Bytes)\n",
      "peakExecutionMemory => 205586176\n",
      "recordsRead => 0\n",
      "bytesRead => 0 (0 Bytes)\n",
      "recordsWritten => 0\n",
      "bytesWritten => 0 (0 Bytes)\n",
      "shuffleRecordsRead => 311068\n",
      "shuffleTotalBlocksFetched => 73\n",
      "shuffleLocalBlocksFetched => 73\n",
      "shuffleRemoteBlocksFetched => 0\n",
      "shuffleTotalBytesRead => 4207403 (4,0 MB)\n",
      "shuffleLocalBytesRead => 4207403 (4,0 MB)\n",
      "shuffleRemoteBytesRead => 0 (0 Bytes)\n",
      "shuffleRemoteBytesReadToDisk => 0 (0 Bytes)\n",
      "shuffleBytesWritten => 1402507 (1369,6 KB)\n",
      "shuffleRecordsWritten => 103690\n",
      "\n",
      "Stages and their duration:\n",
      "Stage 48 duration => 826 (0,8 s)\n",
      "Stage 50 duration => 94 (94 ms)\n",
      "Stage 51 duration => 96 (96 ms)\n",
      "Stage 53 duration => 1943 (2 s)\n",
      "Stage 56 duration => 6 (6 ms)\n"
     ]
    }
   ],
   "source": [
    "stagemetrics.print_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Additional stage-level executor metrics (memory usage info):\n",
      "\n",
      "Stage 48 JVMHeapMemory maxVal bytes => 494025688 (471,1 MB)\n",
      "Stage 48 OnHeapExecutionMemory maxVal bytes => 0 (0 Bytes)\n",
      "Stage 50 JVMHeapMemory maxVal bytes => 494025688 (471,1 MB)\n",
      "Stage 50 OnHeapExecutionMemory maxVal bytes => 0 (0 Bytes)\n",
      "Stage 51 JVMHeapMemory maxVal bytes => 494025688 (471,1 MB)\n",
      "Stage 51 OnHeapExecutionMemory maxVal bytes => 0 (0 Bytes)\n",
      "Stage 53 JVMHeapMemory maxVal bytes => 494025688 (471,1 MB)\n",
      "Stage 53 OnHeapExecutionMemory maxVal bytes => 0 (0 Bytes)\n",
      "Stage 56 JVMHeapMemory maxVal bytes => 494025688 (471,1 MB)\n",
      "Stage 56 OnHeapExecutionMemory maxVal bytes => 0 (0 Bytes)\n"
     ]
    }
   ],
   "source": [
    "stagemetrics.print_memory_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confront with another implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = GraphFrame(vertices,edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "stagemetrics = StageMetrics(spark)\n",
    "stagemetrics.begin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "triangles = graph.triangleCount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "|count| id|\n",
      "+-----+---+\n",
      "|  197| 12|\n",
      "|    0| 22|\n",
      "|   12| 13|\n",
      "| 3143|  6|\n",
      "|   23| 16|\n",
      "|  280|  3|\n",
      "| 1078| 20|\n",
      "|   88|  5|\n",
      "|  482| 19|\n",
      "| 4847| 15|\n",
      "|  321|  9|\n",
      "|  156| 17|\n",
      "|   95|  4|\n",
      "| 2294|  8|\n",
      "|  661| 23|\n",
      "|   42|  7|\n",
      "|  700| 10|\n",
      "|  164| 21|\n",
      "|13401| 11|\n",
      "|  361| 14|\n",
      "+-----+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "triangles.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "| count|\n",
      "+------+\n",
      "|608389|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import sum\n",
    "\n",
    "triangle_count = triangles.select(sum(\"count\")/3)\n",
    "triangle_count.select(col('(sum(count) / 3)').cast(IntegerType()).alias('count')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "stagemetrics.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [CASE WHEN isnull(count#403L) THEN 0 ELSE count#403L END AS count#413L, id#238]\n",
      "   +- SortMergeJoin [id#238], [id#399], LeftOuter\n",
      "      :- Sort [id#238 ASC NULLS FIRST], false, 0\n",
      "      :  +- Exchange hashpartitioning(id#238, 200), ENSURE_REQUIREMENTS, [plan_id=9165]\n",
      "      :     +- Project [value#236 AS id#238]\n",
      "      :        +- Scan ExistingRDD[value#236]\n",
      "      +- Sort [id#399 ASC NULLS FIRST], false, 0\n",
      "         +- HashAggregate(keys=[id#399], functions=[count(1)])\n",
      "            +- Exchange hashpartitioning(id#399, 200), ENSURE_REQUIREMENTS, [plan_id=9161]\n",
      "               +- HashAggregate(keys=[id#399], functions=[partial_count(1)])\n",
      "                  +- Filter isnotnull(id#399)\n",
      "                     +- Generate explode(array(a#320.id, b#322.id, c#343.id)), false, [id#399]\n",
      "                        +- Project [a#320, b#322, c#343]\n",
      "                           +- SortMergeJoin [cast(a#320.id as bigint), cast(c#343.id as bigint)], [__tmp-5059293437631245208#371.src, __tmp-5059293437631245208#371.dst], Inner\n",
      "                              :- Sort [cast(a#320.id as bigint) ASC NULLS FIRST, cast(c#343.id as bigint) ASC NULLS FIRST], false, 0\n",
      "                              :  +- Exchange hashpartitioning(cast(a#320.id as bigint), cast(c#343.id as bigint), 200), ENSURE_REQUIREMENTS, [plan_id=9152]\n",
      "                              :     +- Project [a#320, b#322, c#343]\n",
      "                              :        +- SortMergeJoin [_extract_dst#464L], [cast(c#343.id as bigint)], Inner\n",
      "                              :           :- Sort [_extract_dst#464L ASC NULLS FIRST], false, 0\n",
      "                              :           :  +- Exchange hashpartitioning(_extract_dst#464L, 200), ENSURE_REQUIREMENTS, [plan_id=9141]\n",
      "                              :           :     +- Project [a#320, b#322, __tmp-6526019406657860729#341.dst AS _extract_dst#464L]\n",
      "                              :           :        +- SortMergeJoin [cast(b#322.id as bigint)], [__tmp-6526019406657860729#341.src], Inner\n",
      "                              :           :           :- Project [a#320, b#322]\n",
      "                              :           :           :  +- SortMergeJoin [_extract_dst#465L], [cast(b#322.id as bigint)], Inner\n",
      "                              :           :           :     :- Sort [_extract_dst#465L ASC NULLS FIRST], false, 0\n",
      "                              :           :           :     :  +- Exchange hashpartitioning(_extract_dst#465L, 200), ENSURE_REQUIREMENTS, [plan_id=9125]\n",
      "                              :           :           :     :     +- Project [__tmp-430217833014886237#318.dst AS _extract_dst#465L, a#320]\n",
      "                              :           :           :     :        +- SortMergeJoin [__tmp-430217833014886237#318.src], [cast(a#320.id as bigint)], Inner\n",
      "                              :           :           :     :           :- Sort [__tmp-430217833014886237#318.src ASC NULLS FIRST], false, 0\n",
      "                              :           :           :     :           :  +- Exchange hashpartitioning(__tmp-430217833014886237#318.src, 200), ENSURE_REQUIREMENTS, [plan_id=9117]\n",
      "                              :           :           :     :           :     +- HashAggregate(keys=[src#314L, dst#315L], functions=[])\n",
      "                              :           :           :     :           :        +- Exchange hashpartitioning(src#314L, dst#315L, 200), ENSURE_REQUIREMENTS, [plan_id=9113]\n",
      "                              :           :           :     :           :           +- HashAggregate(keys=[src#314L, dst#315L], functions=[])\n",
      "                              :           :           :     :           :              +- Union\n",
      "                              :           :           :     :           :                 :- Project [if ((src#240L < dst#241L)) src#240L else dst#241L AS src#314L, if ((src#240L < dst#241L)) dst#241L else src#240L AS dst#315L]\n",
      "                              :           :           :     :           :                 :  +- Filter (((isnotnull(src#240L) AND isnotnull(dst#241L)) AND ((src#240L < dst#241L) AND NOT (src#240L = dst#241L))) AND (isnotnull(if ((src#240L < dst#241L)) src#240L else dst#241L) AND isnotnull(if ((src#240L < dst#241L)) dst#241L else src#240L)))\n",
      "                              :           :           :     :           :                 :     +- Scan ExistingRDD[src#240L,dst#241L]\n",
      "                              :           :           :     :           :                 +- Project [if ((dst#249L < src#248L)) dst#249L else src#248L AS src#466L, if ((dst#249L < src#248L)) src#248L else dst#249L AS dst#467L]\n",
      "                              :           :           :     :           :                    +- Filter (((isnotnull(src#248L) AND isnotnull(dst#249L)) AND ((src#248L > dst#249L) AND NOT (dst#249L = src#248L))) AND (isnotnull(if ((dst#249L < src#248L)) dst#249L else src#248L) AND isnotnull(if ((dst#249L < src#248L)) src#248L else dst#249L)))\n",
      "                              :           :           :     :           :                       +- Scan ExistingRDD[src#248L,dst#249L]\n",
      "                              :           :           :     :           +- Sort [cast(a#320.id as bigint) ASC NULLS FIRST], false, 0\n",
      "                              :           :           :     :              +- Exchange hashpartitioning(cast(a#320.id as bigint), 200), ENSURE_REQUIREMENTS, [plan_id=9118]\n",
      "                              :           :           :     :                 +- Project [struct(id, value#406) AS a#320]\n",
      "                              :           :           :     :                    +- Filter isnotnull(value#406)\n",
      "                              :           :           :     :                       +- Scan ExistingRDD[value#406]\n",
      "                              :           :           :     +- Sort [cast(b#322.id as bigint) ASC NULLS FIRST], false, 0\n",
      "                              :           :           :        +- Exchange hashpartitioning(cast(b#322.id as bigint), 200), ENSURE_REQUIREMENTS, [plan_id=9126]\n",
      "                              :           :           :           +- Project [struct(id, value#332) AS b#322]\n",
      "                              :           :           :              +- Filter isnotnull(value#332)\n",
      "                              :           :           :                 +- Scan ExistingRDD[value#332]\n",
      "                              :           :           +- Sort [__tmp-6526019406657860729#341.src ASC NULLS FIRST], false, 0\n",
      "                              :           :              +- Exchange hashpartitioning(__tmp-6526019406657860729#341.src, 200), ENSURE_REQUIREMENTS, [plan_id=9135]\n",
      "                              :           :                 +- HashAggregate(keys=[src#314L, dst#315L], functions=[])\n",
      "                              :           :                    +- Exchange hashpartitioning(src#314L, dst#315L, 200), ENSURE_REQUIREMENTS, [plan_id=9131]\n",
      "                              :           :                       +- HashAggregate(keys=[src#314L, dst#315L], functions=[])\n",
      "                              :           :                          +- Union\n",
      "                              :           :                             :- Project [if ((src#347L < dst#348L)) src#347L else dst#348L AS src#314L, if ((src#347L < dst#348L)) dst#348L else src#347L AS dst#315L]\n",
      "                              :           :                             :  +- Filter (((isnotnull(src#347L) AND isnotnull(dst#348L)) AND ((src#347L < dst#348L) AND NOT (src#347L = dst#348L))) AND (isnotnull(if ((src#347L < dst#348L)) src#347L else dst#348L) AND isnotnull(if ((src#347L < dst#348L)) dst#348L else src#347L)))\n",
      "                              :           :                             :     +- Scan ExistingRDD[src#347L,dst#348L]\n",
      "                              :           :                             +- Project [if ((dst#350L < src#349L)) dst#350L else src#349L AS src#468L, if ((dst#350L < src#349L)) src#349L else dst#350L AS dst#469L]\n",
      "                              :           :                                +- Filter (((isnotnull(src#349L) AND isnotnull(dst#350L)) AND ((src#349L > dst#350L) AND NOT (dst#350L = src#349L))) AND (isnotnull(if ((dst#350L < src#349L)) dst#350L else src#349L) AND isnotnull(if ((dst#350L < src#349L)) src#349L else dst#350L)))\n",
      "                              :           :                                   +- Scan ExistingRDD[src#349L,dst#350L]\n",
      "                              :           +- Sort [cast(c#343.id as bigint) ASC NULLS FIRST], false, 0\n",
      "                              :              +- Exchange hashpartitioning(cast(c#343.id as bigint), 200), ENSURE_REQUIREMENTS, [plan_id=9142]\n",
      "                              :                 +- Project [struct(id, value#359) AS c#343]\n",
      "                              :                    +- Filter isnotnull(value#359)\n",
      "                              :                       +- Scan ExistingRDD[value#359]\n",
      "                              +- Sort [__tmp-5059293437631245208#371.src ASC NULLS FIRST, __tmp-5059293437631245208#371.dst ASC NULLS FIRST], false, 0\n",
      "                                 +- Exchange hashpartitioning(__tmp-5059293437631245208#371.src, __tmp-5059293437631245208#371.dst, 200), ENSURE_REQUIREMENTS, [plan_id=9151]\n",
      "                                    +- HashAggregate(keys=[src#314L, dst#315L], functions=[])\n",
      "                                       +- Exchange hashpartitioning(src#314L, dst#315L, 200), ENSURE_REQUIREMENTS, [plan_id=9147]\n",
      "                                          +- HashAggregate(keys=[src#314L, dst#315L], functions=[])\n",
      "                                             +- Union\n",
      "                                                :- Project [if ((src#377L < dst#378L)) src#377L else dst#378L AS src#314L, if ((src#377L < dst#378L)) dst#378L else src#377L AS dst#315L]\n",
      "                                                :  +- Filter (((isnotnull(src#377L) AND isnotnull(dst#378L)) AND ((src#377L < dst#378L) AND NOT (src#377L = dst#378L))) AND (isnotnull(if ((src#377L < dst#378L)) src#377L else dst#378L) AND isnotnull(if ((src#377L < dst#378L)) dst#378L else src#377L)))\n",
      "                                                :     +- Scan ExistingRDD[src#377L,dst#378L]\n",
      "                                                +- Project [if ((dst#380L < src#379L)) dst#380L else src#379L AS src#470L, if ((dst#380L < src#379L)) src#379L else dst#380L AS dst#471L]\n",
      "                                                   +- Filter (((isnotnull(src#379L) AND isnotnull(dst#380L)) AND ((src#379L > dst#380L) AND NOT (dst#380L = src#379L))) AND (isnotnull(if ((dst#380L < src#379L)) dst#380L else src#379L) AND isnotnull(if ((dst#380L < src#379L)) src#379L else dst#380L)))\n",
      "                                                      +- Scan ExistingRDD[src#379L,dst#380L]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "triangles.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scheduling mode = FIFO\n",
      "Spark Context default degree of parallelism = 12\n",
      "\n",
      "Aggregated Spark stage metrics:\n",
      "numStages => 19\n",
      "numTasks => 109\n",
      "elapsedTime => 5180 (5 s)\n",
      "stageDuration => 8620 (9 s)\n",
      "executorRunTime => 36159 (36 s)\n",
      "executorCpuTime => 6009 (6 s)\n",
      "executorDeserializeTime => 386 (0,4 s)\n",
      "executorDeserializeCpuTime => 274 (0,3 s)\n",
      "resultSerializationTime => 89 (89 ms)\n",
      "jvmGCTime => 2723 (3 s)\n",
      "shuffleFetchWaitTime => 0 (0 ms)\n",
      "shuffleWriteTime => 5688 (6 s)\n",
      "resultSize => 860225 (840,1 KB)\n",
      "diskBytesSpilled => 0 (0 Bytes)\n",
      "memoryBytesSpilled => 0 (0 Bytes)\n",
      "peakExecutionMemory => 196745616\n",
      "recordsRead => 0\n",
      "bytesRead => 0 (0 Bytes)\n",
      "recordsWritten => 0\n",
      "bytesWritten => 0 (0 Bytes)\n",
      "shuffleRecordsRead => 458282\n",
      "shuffleTotalBlocksFetched => 159\n",
      "shuffleLocalBlocksFetched => 159\n",
      "shuffleRemoteBlocksFetched => 0\n",
      "shuffleTotalBytesRead => 6649352 (6,3 MB)\n",
      "shuffleLocalBytesRead => 6649352 (6,3 MB)\n",
      "shuffleRemoteBytesRead => 0 (0 Bytes)\n",
      "shuffleRemoteBytesReadToDisk => 0 (0 Bytes)\n",
      "shuffleBytesWritten => 3488562 (3,3 MB)\n",
      "shuffleRecordsWritten => 236926\n",
      "\n",
      "Stages and their duration:\n",
      "Stage 57 duration => 428 (0,4 s)\n",
      "Stage 58 duration => 1517 (2 s)\n",
      "Stage 59 duration => 1670 (2 s)\n",
      "Stage 61 duration => 122 (0,1 s)\n",
      "Stage 62 duration => 107 (0,1 s)\n",
      "Stage 64 duration => 29 (29 ms)\n",
      "Stage 66 duration => 737 (0,7 s)\n",
      "Stage 69 duration => 13 (13 ms)\n",
      "Stage 71 duration => 6 (6 ms)\n",
      "Stage 72 duration => 450 (0,5 s)\n",
      "Stage 73 duration => 1143 (1 s)\n",
      "Stage 74 duration => 1431 (1 s)\n",
      "Stage 76 duration => 151 (0,2 s)\n",
      "Stage 77 duration => 128 (0,1 s)\n",
      "Stage 79 duration => 22 (22 ms)\n",
      "Stage 81 duration => 622 (0,6 s)\n",
      "Stage 84 duration => 13 (13 ms)\n",
      "Stage 86 duration => 18 (18 ms)\n",
      "Stage 89 duration => 13 (13 ms)\n"
     ]
    }
   ],
   "source": [
    "stagemetrics.print_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Additional stage-level executor metrics (memory usage info):\n",
      "\n",
      "Stage 57 JVMHeapMemory maxVal bytes => 313968720 (299,4 MB)\n",
      "Stage 57 OnHeapExecutionMemory maxVal bytes => 0 (0 Bytes)\n",
      "Stage 58 JVMHeapMemory maxVal bytes => 313968720 (299,4 MB)\n",
      "Stage 58 OnHeapExecutionMemory maxVal bytes => 0 (0 Bytes)\n",
      "Stage 59 JVMHeapMemory maxVal bytes => 313968720 (299,4 MB)\n",
      "Stage 59 OnHeapExecutionMemory maxVal bytes => 0 (0 Bytes)\n",
      "Stage 61 JVMHeapMemory maxVal bytes => 313968720 (299,4 MB)\n",
      "Stage 61 OnHeapExecutionMemory maxVal bytes => 0 (0 Bytes)\n",
      "Stage 62 JVMHeapMemory maxVal bytes => 313968720 (299,4 MB)\n",
      "Stage 62 OnHeapExecutionMemory maxVal bytes => 0 (0 Bytes)\n",
      "Stage 64 JVMHeapMemory maxVal bytes => 313968720 (299,4 MB)\n",
      "Stage 64 OnHeapExecutionMemory maxVal bytes => 0 (0 Bytes)\n",
      "Stage 66 JVMHeapMemory maxVal bytes => 313968720 (299,4 MB)\n",
      "Stage 66 OnHeapExecutionMemory maxVal bytes => 0 (0 Bytes)\n",
      "Stage 69 JVMHeapMemory maxVal bytes => 313968720 (299,4 MB)\n",
      "Stage 69 OnHeapExecutionMemory maxVal bytes => 0 (0 Bytes)\n",
      "Stage 71 JVMHeapMemory maxVal bytes => 313968720 (299,4 MB)\n",
      "Stage 71 OnHeapExecutionMemory maxVal bytes => 0 (0 Bytes)\n",
      "Stage 72 JVMHeapMemory maxVal bytes => 313968720 (299,4 MB)\n",
      "Stage 72 OnHeapExecutionMemory maxVal bytes => 0 (0 Bytes)\n",
      "Stage 73 JVMHeapMemory maxVal bytes => 313968720 (299,4 MB)\n",
      "Stage 73 OnHeapExecutionMemory maxVal bytes => 0 (0 Bytes)\n",
      "Stage 74 JVMHeapMemory maxVal bytes => 313968720 (299,4 MB)\n",
      "Stage 74 OnHeapExecutionMemory maxVal bytes => 0 (0 Bytes)\n",
      "Stage 76 JVMHeapMemory maxVal bytes => 313968720 (299,4 MB)\n",
      "Stage 76 OnHeapExecutionMemory maxVal bytes => 0 (0 Bytes)\n",
      "Stage 77 JVMHeapMemory maxVal bytes => 313968720 (299,4 MB)\n",
      "Stage 77 OnHeapExecutionMemory maxVal bytes => 0 (0 Bytes)\n",
      "Stage 79 JVMHeapMemory maxVal bytes => 313968720 (299,4 MB)\n",
      "Stage 79 OnHeapExecutionMemory maxVal bytes => 0 (0 Bytes)\n",
      "Stage 81 JVMHeapMemory maxVal bytes => 313968720 (299,4 MB)\n",
      "Stage 81 OnHeapExecutionMemory maxVal bytes => 0 (0 Bytes)\n",
      "Stage 84 JVMHeapMemory maxVal bytes => 313968720 (299,4 MB)\n",
      "Stage 84 OnHeapExecutionMemory maxVal bytes => 0 (0 Bytes)\n",
      "Stage 86 JVMHeapMemory maxVal bytes => 313968720 (299,4 MB)\n",
      "Stage 86 OnHeapExecutionMemory maxVal bytes => 0 (0 Bytes)\n",
      "Stage 89 JVMHeapMemory maxVal bytes => 313968720 (299,4 MB)\n",
      "Stage 89 OnHeapExecutionMemory maxVal bytes => 0 (0 Bytes)\n"
     ]
    }
   ],
   "source": [
    "stagemetrics.print_memory_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
