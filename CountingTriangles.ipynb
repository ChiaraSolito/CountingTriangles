{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SetUp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "\n",
    "os.environ['SPARK_HOME'] = '/home/chiara/Documenti/BigData/CountingTriangles/spark-3.5.0-bin-hadoop3'\n",
    "os.environ['HADOOP_HOME'] = '/home/chiara/Documenti/BigData/CountingTriangles/spark-3.5.0-bin-hadoop3'\n",
    "os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-17-openjdk-amd64'\n",
    "os.environ['SPARK_LOCAL_IP'] = '172.17.0.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_python = os.path.join(os.environ.get('SPARK_HOME',None),'python')\n",
    "py4j = glob.glob(os.path.join(spark_python,'lib','py4j-*.zip'))[0]\n",
    "graphf = glob.glob(os.path.join(spark_python,'graphframes.zip'))[0]\n",
    "sparkmeasure = glob.glob(os.path.join(spark_python,'sparkmeasure.zip'))[0]\n",
    "sys.path[:0]=[spark_python,py4j]\n",
    "sys.path[:0]=[spark_python,graphf]\n",
    "sys.path[:0]=[spark_python, sparkmeasure]\n",
    "os.environ['PYTHONPATH']=py4j+os.pathsep+graphf+sparkmeasure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from graphframes import *\n",
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphframes import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import IntegerType, Row\n",
    "from pyspark.sql.functions import col\n",
    "from sparkmeasure import StageMetrics, TaskMetrics\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/chiara/Documenti/BigData/CountingTriangles/spark-3.5.0-bin-hadoop3'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/02/26 12:30:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = (SparkSession.builder\n",
    "    .appName('Counting Triangles')\n",
    "    .config('spark.driver.extraClassPath', '/usr/local/bin/postgresql-42.2.5.jar')\n",
    "    .config('spark.executor.memory', '8g')\n",
    "    .config(\"spark.driver.memory\", \"8g\")\n",
    "    .config('spark.memory.offHeap.enabled', True)\n",
    "    .config('spark.memory.offHeap.size', '20g') \n",
    "    .enableHiveSupport()\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'datasets/soc-Epinions1.txt'\n",
    "with open(dataset,'r') as f:\n",
    "    content = f.readlines()\n",
    "edges_list = list(filter( lambda x: not x.startswith('#') ,content))\n",
    "\n",
    "\n",
    "if 'facebook' in dataset:\n",
    "    edges = list(map(lambda x: tuple(x.split(' ')), edges_list))\n",
    "else:\n",
    "    edges = list(map(lambda x: tuple(x.split('\\t')), edges_list))\n",
    "\n",
    "edges_tuples = list(map(lambda x: (int(x[0]), int(x[1].replace('\\n',''))), edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1, list2 = zip(*edges_tuples)\n",
    "nodes = list(set(list1 + list2))\n",
    "nodes_tuple = [Row(x) for x in nodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get list of nodes, with columns renamed value and id\n",
    "vertices = spark.createDataFrame(nodes, IntegerType())\n",
    "vertices = vertices.withColumnRenamed('value','id')\n",
    "\n",
    "#get edges such that the src node is always smaller then the dst node\n",
    "edges_n = spark.createDataFrame(edges_tuples,[\"src\", \"dst\"],IntegerType())\n",
    "edges_inverted = edges_n.filter(edges_n.src > edges_n.dst)\n",
    "edges_normal = edges_n.filter(edges_n.src < edges_n.dst)\n",
    "edges_normal2 = edges_inverted.select(col('dst').alias('src'),col('src').alias('dst'))\n",
    "edges_true = edges_normal.union(edges_normal2)\n",
    "edges = edges_normal.union(edges_normal2).distinct()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check we read the right data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "75879"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vertices.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The edges used are not the same length as referred in the dataset page, this is because of the distinct() operation at the end of the selection of edges: we treat each graph as undirected and only consider edges where srcId < dstId, so when we encounter two edges of the kind (srcId, dstId) (dstId, srcId), with srcId < dstId or viceversa, we're going to keep just one edge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "508837"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges_true.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "405740"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stagemetrics = StageMetrics(spark)\n",
    "taskmetrics = TaskMetrics(spark)\n",
    "stagemetrics.begin()\n",
    "taskmetrics.begin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = edges.alias(\"e1\")\\\n",
    "    .join(edges.alias(\"e2\"), col(\"e1.src\") == col(\"e2.src\")) \\\n",
    "    .join(edges.alias(\"e3\"), (col(\"e1.dst\") == col(\"e3.src\")) & (col(\"e2.dst\") == col(\"e3.dst\"))) \\\n",
    "    .select(col(\"e1.src\").alias(\"node1\"), col(\"e1.dst\").alias(\"node2\"), col(\"e2.dst\").alias(\"node3\")).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1624481"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "stagemetrics.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "taskmetrics.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[node1#68L, node2#69L, node3#70L], functions=[])\n",
      "   +- HashAggregate(keys=[node1#68L, node2#69L, node3#70L], functions=[])\n",
      "      +- Project [src#4L AS node1#68L, dst#5L AS node2#69L, dst#41L AS node3#70L]\n",
      "         +- SortMergeJoin [dst#5L, dst#41L], [src#52L, dst#53L], Inner\n",
      "            :- Sort [dst#5L ASC NULLS FIRST, dst#41L ASC NULLS FIRST], false, 0\n",
      "            :  +- Exchange hashpartitioning(dst#5L, dst#41L, 200), ENSURE_REQUIREMENTS, [plan_id=1159]\n",
      "            :     +- Project [src#4L, dst#5L, dst#41L]\n",
      "            :        +- SortMergeJoin [src#4L], [src#40L], Inner\n",
      "            :           :- Sort [src#4L ASC NULLS FIRST], false, 0\n",
      "            :           :  +- Exchange hashpartitioning(src#4L, 200), ENSURE_REQUIREMENTS, [plan_id=1149]\n",
      "            :           :     +- HashAggregate(keys=[src#4L, dst#5L], functions=[])\n",
      "            :           :        +- Exchange hashpartitioning(src#4L, dst#5L, 200), ENSURE_REQUIREMENTS, [plan_id=1143]\n",
      "            :           :           +- HashAggregate(keys=[src#4L, dst#5L], functions=[])\n",
      "            :           :              +- Union\n",
      "            :           :                 :- Filter ((isnotnull(src#4L) AND isnotnull(dst#5L)) AND (src#4L < dst#5L))\n",
      "            :           :                 :  +- Scan ExistingRDD[src#4L,dst#5L]\n",
      "            :           :                 +- Project [dst#17L AS src#8L, src#16L AS dst#9L]\n",
      "            :           :                    +- Filter ((isnotnull(src#16L) AND isnotnull(dst#17L)) AND (src#16L > dst#17L))\n",
      "            :           :                       +- Scan ExistingRDD[src#16L,dst#17L]\n",
      "            :           +- Sort [src#40L ASC NULLS FIRST], false, 0\n",
      "            :              +- Exchange hashpartitioning(src#40L, 200), ENSURE_REQUIREMENTS, [plan_id=1150]\n",
      "            :                 +- HashAggregate(keys=[src#40L, dst#41L], functions=[])\n",
      "            :                    +- Exchange hashpartitioning(src#40L, dst#41L, 200), ENSURE_REQUIREMENTS, [plan_id=1145]\n",
      "            :                       +- HashAggregate(keys=[src#40L, dst#41L], functions=[])\n",
      "            :                          +- Union\n",
      "            :                             :- Filter ((isnotnull(src#40L) AND isnotnull(dst#41L)) AND (src#40L < dst#41L))\n",
      "            :                             :  +- Scan ExistingRDD[src#40L,dst#41L]\n",
      "            :                             +- Project [dst#43L AS src#8L, src#42L AS dst#9L]\n",
      "            :                                +- Filter ((isnotnull(src#42L) AND isnotnull(dst#43L)) AND (src#42L > dst#43L))\n",
      "            :                                   +- Scan ExistingRDD[src#42L,dst#43L]\n",
      "            +- Sort [src#52L ASC NULLS FIRST, dst#53L ASC NULLS FIRST], false, 0\n",
      "               +- HashAggregate(keys=[src#52L, dst#53L], functions=[])\n",
      "                  +- Exchange hashpartitioning(src#52L, dst#53L, 200), ENSURE_REQUIREMENTS, [plan_id=1155]\n",
      "                     +- HashAggregate(keys=[src#52L, dst#53L], functions=[])\n",
      "                        +- Union\n",
      "                           :- Filter ((isnotnull(src#52L) AND isnotnull(dst#53L)) AND (src#52L < dst#53L))\n",
      "                           :  +- Scan ExistingRDD[src#52L,dst#53L]\n",
      "                           +- Project [dst#55L AS src#8L, src#54L AS dst#9L]\n",
      "                              +- Filter ((isnotnull(src#54L) AND isnotnull(dst#55L)) AND (src#54L > dst#55L))\n",
      "                                 +- Scan ExistingRDD[src#54L,dst#55L]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure the performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scheduling mode = FIFO\n",
      "Spark Context default degree of parallelism = 12\n",
      "\n",
      "Aggregated Spark stage metrics:\n",
      "numStages => 6\n",
      "numTasks => 53\n",
      "elapsedTime => 35163 (35 s)\n",
      "stageDuration => 34659 (35 s)\n",
      "executorRunTime => 247540 (4,1 min)\n",
      "executorCpuTime => 224467 (3,7 min)\n",
      "executorDeserializeTime => 403 (0,4 s)\n",
      "executorDeserializeCpuTime => 298 (0,3 s)\n",
      "resultSerializationTime => 22 (22 ms)\n",
      "jvmGCTime => 1074 (1 s)\n",
      "shuffleFetchWaitTime => 7 (7 ms)\n",
      "shuffleWriteTime => 4967 (5 s)\n",
      "resultSize => 3694599 (3,5 MB)\n",
      "diskBytesSpilled => 0 (0 Bytes)\n",
      "memoryBytesSpilled => 0 (0 Bytes)\n",
      "peakExecutionMemory => 14706526584\n",
      "recordsRead => 0\n",
      "bytesRead => 0 (0 Bytes)\n",
      "recordsWritten => 0\n",
      "bytesWritten => 0 (0 Bytes)\n",
      "shuffleRecordsRead => 91283228\n",
      "shuffleTotalBlocksFetched => 604\n",
      "shuffleLocalBlocksFetched => 604\n",
      "shuffleRemoteBlocksFetched => 0\n",
      "shuffleTotalBytesRead => 936325617 (892,9 MB)\n",
      "shuffleLocalBytesRead => 936325617 (892,9 MB)\n",
      "shuffleRemoteBytesRead => 0 (0 Bytes)\n",
      "shuffleRemoteBytesReadToDisk => 0 (0 Bytes)\n",
      "shuffleBytesWritten => 926068206 (883,2 MB)\n",
      "shuffleRecordsWritten => 90368651\n",
      "\n",
      "Stages and their duration:\n",
      "Stage 12 duration => 1179 (1 s)\n",
      "Stage 14 duration => 358 (0,4 s)\n",
      "Stage 17 duration => 211 (0,2 s)\n",
      "Stage 20 duration => 11111 (11 s)\n",
      "Stage 24 duration => 21786 (22 s)\n",
      "Stage 29 duration => 14 (14 ms)\n"
     ]
    }
   ],
   "source": [
    "stagemetrics.print_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scheduling mode = FIFO\n",
      "Spark Context default degree of parallelism = 12\n",
      "\n",
      "Aggregated Spark task metrics:\n",
      "numTasks => 53\n",
      "successful tasks => 53\n",
      "speculative tasks => 0\n",
      "taskDuration => 248697 (4,1 min)\n",
      "schedulerDelayTime => 437 (0,4 s)\n",
      "executorRunTime => 247540 (4,1 min)\n",
      "executorCpuTime => 224445 (3,7 min)\n",
      "executorDeserializeTime => 403 (0,4 s)\n",
      "executorDeserializeCpuTime => 275 (0,3 s)\n",
      "resultSerializationTime => 22 (22 ms)\n",
      "jvmGCTime => 1074 (1 s)\n",
      "shuffleFetchWaitTime => 7 (7 ms)\n",
      "shuffleWriteTime => 4950 (5 s)\n",
      "gettingResultTime => 295 (0,3 s)\n",
      "resultSize => 1291046 (1260,8 KB)\n",
      "diskBytesSpilled => 0 (0 Bytes)\n",
      "memoryBytesSpilled => 0 (0 Bytes)\n",
      "peakExecutionMemory => 14706526584\n",
      "recordsRead => 0\n",
      "bytesRead => 0 (0 Bytes)\n",
      "recordsWritten => 0\n",
      "bytesWritten => 0 (0 Bytes)\n",
      "shuffleRecordsRead => 91283228\n",
      "shuffleTotalBlocksFetched => 604\n",
      "shuffleLocalBlocksFetched => 604\n",
      "shuffleRemoteBlocksFetched => 0\n",
      "shuffleTotalBytesRead => 936325617 (892,9 MB)\n",
      "shuffleLocalBytesRead => 936325617 (892,9 MB)\n",
      "shuffleRemoteBytesRead => 0 (0 Bytes)\n",
      "shuffleRemoteBytesReadToDisk => 0 (0 Bytes)\n",
      "shuffleBytesWritten => 926068206 (883,2 MB)\n",
      "shuffleRecordsWritten => 90368651\n"
     ]
    }
   ],
   "source": [
    "taskmetrics.print_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Additional stage-level executor metrics (memory usage info):\n",
      "\n",
      "Stage 12 JVMHeapMemory maxVal bytes => 2007194112 (1914,2 MB)\n",
      "Stage 12 OnHeapExecutionMemory maxVal bytes => 0 (0 Bytes)\n",
      "Stage 14 JVMHeapMemory maxVal bytes => 2007194112 (1914,2 MB)\n",
      "Stage 14 OnHeapExecutionMemory maxVal bytes => 0 (0 Bytes)\n",
      "Stage 17 JVMHeapMemory maxVal bytes => 2007194112 (1914,2 MB)\n",
      "Stage 17 OnHeapExecutionMemory maxVal bytes => 0 (0 Bytes)\n",
      "Stage 20 JVMHeapMemory maxVal bytes => 2007194112 (1914,2 MB)\n",
      "Stage 20 OnHeapExecutionMemory maxVal bytes => 0 (0 Bytes)\n",
      "Stage 24 JVMHeapMemory maxVal bytes => 1734564352 (1654,2 MB)\n",
      "Stage 24 OnHeapExecutionMemory maxVal bytes => 0 (0 Bytes)\n",
      "Stage 29 JVMHeapMemory maxVal bytes => 1734564352 (1654,2 MB)\n",
      "Stage 29 OnHeapExecutionMemory maxVal bytes => 0 (0 Bytes)\n"
     ]
    }
   ],
   "source": [
    "stagemetrics.print_memory_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confront with another implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = GraphFrame(vertices,edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "stagemetrics = StageMetrics(spark)\n",
    "taskmetrics = TaskMetrics(spark)\n",
    "stagemetrics.begin()\n",
    "taskmetrics.begin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "triangles = graph.triangleCount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|  count|\n",
      "+-------+\n",
      "|1624481|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "triangle_count = triangles.select(sum(\"count\")/3)\n",
    "triangle_count.select(col('(sum(count) / 3)').cast(IntegerType()).alias('count')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "stagemetrics.end()\n",
    "taskmetrics.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [CASE WHEN isnull(count#182L) THEN 0 ELSE count#182L END AS count#192L, id#2]\n",
      "   +- SortMergeJoin [id#2], [id#178], LeftOuter\n",
      "      :- Sort [id#2 ASC NULLS FIRST], false, 0\n",
      "      :  +- Exchange hashpartitioning(id#2, 200), ENSURE_REQUIREMENTS, [plan_id=3731]\n",
      "      :     +- Project [value#0 AS id#2]\n",
      "      :        +- Scan ExistingRDD[value#0]\n",
      "      +- Sort [id#178 ASC NULLS FIRST], false, 0\n",
      "         +- HashAggregate(keys=[id#178], functions=[count(1)])\n",
      "            +- Exchange hashpartitioning(id#178, 200), ENSURE_REQUIREMENTS, [plan_id=3727]\n",
      "               +- HashAggregate(keys=[id#178], functions=[partial_count(1)])\n",
      "                  +- Filter isnotnull(id#178)\n",
      "                     +- Generate explode(array(a#99.id, b#101.id, c#122.id)), false, [id#178]\n",
      "                        +- Project [a#99, b#101, c#122]\n",
      "                           +- SortMergeJoin [cast(a#99.id as bigint), cast(c#122.id as bigint)], [__tmp-1043886091038848698#150.src, __tmp-1043886091038848698#150.dst], Inner\n",
      "                              :- Sort [cast(a#99.id as bigint) ASC NULLS FIRST, cast(c#122.id as bigint) ASC NULLS FIRST], false, 0\n",
      "                              :  +- Exchange hashpartitioning(cast(a#99.id as bigint), cast(c#122.id as bigint), 200), ENSURE_REQUIREMENTS, [plan_id=3718]\n",
      "                              :     +- Project [a#99, b#101, c#122]\n",
      "                              :        +- SortMergeJoin [_extract_dst#227L], [cast(c#122.id as bigint)], Inner\n",
      "                              :           :- Sort [_extract_dst#227L ASC NULLS FIRST], false, 0\n",
      "                              :           :  +- Exchange hashpartitioning(_extract_dst#227L, 200), ENSURE_REQUIREMENTS, [plan_id=3707]\n",
      "                              :           :     +- Project [a#99, b#101, __tmp3640351034883199571#120.dst AS _extract_dst#227L]\n",
      "                              :           :        +- SortMergeJoin [cast(b#101.id as bigint)], [__tmp3640351034883199571#120.src], Inner\n",
      "                              :           :           :- Project [a#99, b#101]\n",
      "                              :           :           :  +- SortMergeJoin [_extract_dst#228L], [cast(b#101.id as bigint)], Inner\n",
      "                              :           :           :     :- Sort [_extract_dst#228L ASC NULLS FIRST], false, 0\n",
      "                              :           :           :     :  +- Exchange hashpartitioning(_extract_dst#228L, 200), ENSURE_REQUIREMENTS, [plan_id=3691]\n",
      "                              :           :           :     :     +- Project [__tmp-4363599943432734077#97.dst AS _extract_dst#228L, a#99]\n",
      "                              :           :           :     :        +- SortMergeJoin [__tmp-4363599943432734077#97.src], [cast(a#99.id as bigint)], Inner\n",
      "                              :           :           :     :           :- Sort [__tmp-4363599943432734077#97.src ASC NULLS FIRST], false, 0\n",
      "                              :           :           :     :           :  +- Exchange hashpartitioning(__tmp-4363599943432734077#97.src, 200), ENSURE_REQUIREMENTS, [plan_id=3683]\n",
      "                              :           :           :     :           :     +- HashAggregate(keys=[src#93L, dst#94L], functions=[])\n",
      "                              :           :           :     :           :        +- Exchange hashpartitioning(src#93L, dst#94L, 200), ENSURE_REQUIREMENTS, [plan_id=3679]\n",
      "                              :           :           :     :           :           +- HashAggregate(keys=[src#93L, dst#94L], functions=[])\n",
      "                              :           :           :     :           :              +- Union\n",
      "                              :           :           :     :           :                 :- Project [if ((src#4L < dst#5L)) src#4L else dst#5L AS src#93L, if ((src#4L < dst#5L)) dst#5L else src#4L AS dst#94L]\n",
      "                              :           :           :     :           :                 :  +- Filter (((isnotnull(src#4L) AND isnotnull(dst#5L)) AND ((src#4L < dst#5L) AND NOT (src#4L = dst#5L))) AND (isnotnull(if ((src#4L < dst#5L)) src#4L else dst#5L) AND isnotnull(if ((src#4L < dst#5L)) dst#5L else src#4L)))\n",
      "                              :           :           :     :           :                 :     +- Scan ExistingRDD[src#4L,dst#5L]\n",
      "                              :           :           :     :           :                 +- Project [if ((dst#17L < src#16L)) dst#17L else src#16L AS src#229L, if ((dst#17L < src#16L)) src#16L else dst#17L AS dst#230L]\n",
      "                              :           :           :     :           :                    +- Filter (((isnotnull(src#16L) AND isnotnull(dst#17L)) AND ((src#16L > dst#17L) AND NOT (dst#17L = src#16L))) AND (isnotnull(if ((dst#17L < src#16L)) dst#17L else src#16L) AND isnotnull(if ((dst#17L < src#16L)) src#16L else dst#17L)))\n",
      "                              :           :           :     :           :                       +- Scan ExistingRDD[src#16L,dst#17L]\n",
      "                              :           :           :     :           +- Sort [cast(a#99.id as bigint) ASC NULLS FIRST], false, 0\n",
      "                              :           :           :     :              +- Exchange hashpartitioning(cast(a#99.id as bigint), 200), ENSURE_REQUIREMENTS, [plan_id=3684]\n",
      "                              :           :           :     :                 +- Project [struct(id, value#185) AS a#99]\n",
      "                              :           :           :     :                    +- Filter isnotnull(value#185)\n",
      "                              :           :           :     :                       +- Scan ExistingRDD[value#185]\n",
      "                              :           :           :     +- Sort [cast(b#101.id as bigint) ASC NULLS FIRST], false, 0\n",
      "                              :           :           :        +- Exchange hashpartitioning(cast(b#101.id as bigint), 200), ENSURE_REQUIREMENTS, [plan_id=3692]\n",
      "                              :           :           :           +- Project [struct(id, value#111) AS b#101]\n",
      "                              :           :           :              +- Filter isnotnull(value#111)\n",
      "                              :           :           :                 +- Scan ExistingRDD[value#111]\n",
      "                              :           :           +- Sort [__tmp3640351034883199571#120.src ASC NULLS FIRST], false, 0\n",
      "                              :           :              +- Exchange hashpartitioning(__tmp3640351034883199571#120.src, 200), ENSURE_REQUIREMENTS, [plan_id=3701]\n",
      "                              :           :                 +- HashAggregate(keys=[src#93L, dst#94L], functions=[])\n",
      "                              :           :                    +- Exchange hashpartitioning(src#93L, dst#94L, 200), ENSURE_REQUIREMENTS, [plan_id=3697]\n",
      "                              :           :                       +- HashAggregate(keys=[src#93L, dst#94L], functions=[])\n",
      "                              :           :                          +- Union\n",
      "                              :           :                             :- Project [if ((src#126L < dst#127L)) src#126L else dst#127L AS src#93L, if ((src#126L < dst#127L)) dst#127L else src#126L AS dst#94L]\n",
      "                              :           :                             :  +- Filter (((isnotnull(src#126L) AND isnotnull(dst#127L)) AND ((src#126L < dst#127L) AND NOT (src#126L = dst#127L))) AND (isnotnull(if ((src#126L < dst#127L)) src#126L else dst#127L) AND isnotnull(if ((src#126L < dst#127L)) dst#127L else src#126L)))\n",
      "                              :           :                             :     +- Scan ExistingRDD[src#126L,dst#127L]\n",
      "                              :           :                             +- Project [if ((dst#129L < src#128L)) dst#129L else src#128L AS src#231L, if ((dst#129L < src#128L)) src#128L else dst#129L AS dst#232L]\n",
      "                              :           :                                +- Filter (((isnotnull(src#128L) AND isnotnull(dst#129L)) AND ((src#128L > dst#129L) AND NOT (dst#129L = src#128L))) AND (isnotnull(if ((dst#129L < src#128L)) dst#129L else src#128L) AND isnotnull(if ((dst#129L < src#128L)) src#128L else dst#129L)))\n",
      "                              :           :                                   +- Scan ExistingRDD[src#128L,dst#129L]\n",
      "                              :           +- Sort [cast(c#122.id as bigint) ASC NULLS FIRST], false, 0\n",
      "                              :              +- Exchange hashpartitioning(cast(c#122.id as bigint), 200), ENSURE_REQUIREMENTS, [plan_id=3708]\n",
      "                              :                 +- Project [struct(id, value#138) AS c#122]\n",
      "                              :                    +- Filter isnotnull(value#138)\n",
      "                              :                       +- Scan ExistingRDD[value#138]\n",
      "                              +- Sort [__tmp-1043886091038848698#150.src ASC NULLS FIRST, __tmp-1043886091038848698#150.dst ASC NULLS FIRST], false, 0\n",
      "                                 +- Exchange hashpartitioning(__tmp-1043886091038848698#150.src, __tmp-1043886091038848698#150.dst, 200), ENSURE_REQUIREMENTS, [plan_id=3717]\n",
      "                                    +- HashAggregate(keys=[src#93L, dst#94L], functions=[])\n",
      "                                       +- Exchange hashpartitioning(src#93L, dst#94L, 200), ENSURE_REQUIREMENTS, [plan_id=3713]\n",
      "                                          +- HashAggregate(keys=[src#93L, dst#94L], functions=[])\n",
      "                                             +- Union\n",
      "                                                :- Project [if ((src#156L < dst#157L)) src#156L else dst#157L AS src#93L, if ((src#156L < dst#157L)) dst#157L else src#156L AS dst#94L]\n",
      "                                                :  +- Filter (((isnotnull(src#156L) AND isnotnull(dst#157L)) AND ((src#156L < dst#157L) AND NOT (src#156L = dst#157L))) AND (isnotnull(if ((src#156L < dst#157L)) src#156L else dst#157L) AND isnotnull(if ((src#156L < dst#157L)) dst#157L else src#156L)))\n",
      "                                                :     +- Scan ExistingRDD[src#156L,dst#157L]\n",
      "                                                +- Project [if ((dst#159L < src#158L)) dst#159L else src#158L AS src#233L, if ((dst#159L < src#158L)) src#158L else dst#159L AS dst#234L]\n",
      "                                                   +- Filter (((isnotnull(src#158L) AND isnotnull(dst#159L)) AND ((src#158L > dst#159L) AND NOT (dst#159L = src#158L))) AND (isnotnull(if ((dst#159L < src#158L)) dst#159L else src#158L) AND isnotnull(if ((dst#159L < src#158L)) src#158L else dst#159L)))\n",
      "                                                      +- Scan ExistingRDD[src#158L,dst#159L]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "triangles.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scheduling mode = FIFO\n",
      "Spark Context default degree of parallelism = 12\n",
      "\n",
      "Aggregated Spark stage metrics:\n",
      "numStages => 12\n",
      "numTasks => 89\n",
      "elapsedTime => 13481 (13 s)\n",
      "stageDuration => 16382 (16 s)\n",
      "executorRunTime => 126532 (2,1 min)\n",
      "executorCpuTime => 96943 (1,6 min)\n",
      "executorDeserializeTime => 441 (0,4 s)\n",
      "executorDeserializeCpuTime => 332 (0,3 s)\n",
      "resultSerializationTime => 0 (0 ms)\n",
      "jvmGCTime => 916 (0,9 s)\n",
      "shuffleFetchWaitTime => 0 (0 ms)\n",
      "shuffleWriteTime => 6166 (6 s)\n",
      "resultSize => 395143 (385,9 KB)\n",
      "diskBytesSpilled => 0 (0 Bytes)\n",
      "memoryBytesSpilled => 0 (0 Bytes)\n",
      "peakExecutionMemory => 7961934704\n",
      "recordsRead => 0\n",
      "bytesRead => 0 (0 Bytes)\n",
      "recordsWritten => 0\n",
      "bytesWritten => 0 (0 Bytes)\n",
      "shuffleRecordsRead => 24596730\n",
      "shuffleTotalBlocksFetched => 533\n",
      "shuffleLocalBlocksFetched => 533\n",
      "shuffleRemoteBlocksFetched => 0\n",
      "shuffleTotalBytesRead => 289217294 (275,8 MB)\n",
      "shuffleLocalBytesRead => 289217294 (275,8 MB)\n",
      "shuffleRemoteBytesRead => 0 (0 Bytes)\n",
      "shuffleRemoteBytesReadToDisk => 0 (0 Bytes)\n",
      "shuffleBytesWritten => 278751716 (265,8 MB)\n",
      "shuffleRecordsWritten => 23682153\n",
      "\n",
      "Stages and their duration:\n",
      "Stage 30 duration => 689 (0,7 s)\n",
      "Stage 31 duration => 1724 (2 s)\n",
      "Stage 32 duration => 2093 (2 s)\n",
      "Stage 34 duration => 582 (0,6 s)\n",
      "Stage 35 duration => 567 (0,6 s)\n",
      "Stage 37 duration => 243 (0,2 s)\n",
      "Stage 40 duration => 183 (0,2 s)\n",
      "Stage 44 duration => 2995 (3 s)\n",
      "Stage 50 duration => 7145 (7 s)\n",
      "Stage 57 duration => 92 (92 ms)\n",
      "Stage 59 duration => 54 (54 ms)\n",
      "Stage 62 duration => 15 (15 ms)\n"
     ]
    }
   ],
   "source": [
    "stagemetrics.print_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scheduling mode = FIFO\n",
      "Spark Context default degree of parallelism = 12\n",
      "\n",
      "Aggregated Spark task metrics:\n",
      "numTasks => 89\n",
      "successful tasks => 89\n",
      "speculative tasks => 0\n",
      "taskDuration => 127481 (2,1 min)\n",
      "schedulerDelayTime => 508 (0,5 s)\n",
      "executorRunTime => 126532 (2,1 min)\n",
      "executorCpuTime => 96907 (1,6 min)\n",
      "executorDeserializeTime => 441 (0,4 s)\n",
      "executorDeserializeCpuTime => 292 (0,3 s)\n",
      "resultSerializationTime => 0 (0 ms)\n",
      "jvmGCTime => 916 (0,9 s)\n",
      "shuffleFetchWaitTime => 0 (0 ms)\n",
      "shuffleWriteTime => 6127 (6 s)\n",
      "gettingResultTime => 0 (0 ms)\n",
      "resultSize => 395143 (385,9 KB)\n",
      "diskBytesSpilled => 0 (0 Bytes)\n",
      "memoryBytesSpilled => 0 (0 Bytes)\n",
      "peakExecutionMemory => 7961934704\n",
      "recordsRead => 0\n",
      "bytesRead => 0 (0 Bytes)\n",
      "recordsWritten => 0\n",
      "bytesWritten => 0 (0 Bytes)\n",
      "shuffleRecordsRead => 24596730\n",
      "shuffleTotalBlocksFetched => 533\n",
      "shuffleLocalBlocksFetched => 533\n",
      "shuffleRemoteBlocksFetched => 0\n",
      "shuffleTotalBytesRead => 289217294 (275,8 MB)\n",
      "shuffleLocalBytesRead => 289217294 (275,8 MB)\n",
      "shuffleRemoteBytesRead => 0 (0 Bytes)\n",
      "shuffleRemoteBytesReadToDisk => 0 (0 Bytes)\n",
      "shuffleBytesWritten => 278751716 (265,8 MB)\n",
      "shuffleRecordsWritten => 23682153\n"
     ]
    }
   ],
   "source": [
    "taskmetrics.print_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Additional stage-level executor metrics (memory usage info):\n",
      "\n",
      "Stage 30 JVMHeapMemory maxVal bytes => 2426059776 (2,3 GB)\n",
      "Stage 30 OnHeapExecutionMemory maxVal bytes => 0 (0 Bytes)\n",
      "Stage 31 JVMHeapMemory maxVal bytes => 2426059776 (2,3 GB)\n",
      "Stage 31 OnHeapExecutionMemory maxVal bytes => 0 (0 Bytes)\n",
      "Stage 32 JVMHeapMemory maxVal bytes => 2426059776 (2,3 GB)\n",
      "Stage 32 OnHeapExecutionMemory maxVal bytes => 0 (0 Bytes)\n",
      "Stage 34 JVMHeapMemory maxVal bytes => 2426059776 (2,3 GB)\n",
      "Stage 34 OnHeapExecutionMemory maxVal bytes => 0 (0 Bytes)\n",
      "Stage 35 JVMHeapMemory maxVal bytes => 2426059776 (2,3 GB)\n",
      "Stage 35 OnHeapExecutionMemory maxVal bytes => 0 (0 Bytes)\n",
      "Stage 37 JVMHeapMemory maxVal bytes => 2426059776 (2,3 GB)\n",
      "Stage 37 OnHeapExecutionMemory maxVal bytes => 0 (0 Bytes)\n",
      "Stage 40 JVMHeapMemory maxVal bytes => 2426059776 (2,3 GB)\n",
      "Stage 40 OnHeapExecutionMemory maxVal bytes => 0 (0 Bytes)\n",
      "Stage 44 JVMHeapMemory maxVal bytes => 2426059776 (2,3 GB)\n",
      "Stage 44 OnHeapExecutionMemory maxVal bytes => 0 (0 Bytes)\n",
      "Stage 50 JVMHeapMemory maxVal bytes => 2426059776 (2,3 GB)\n",
      "Stage 50 OnHeapExecutionMemory maxVal bytes => 0 (0 Bytes)\n",
      "Stage 57 JVMHeapMemory maxVal bytes => 1963945440 (1873,0 MB)\n",
      "Stage 57 OnHeapExecutionMemory maxVal bytes => 0 (0 Bytes)\n",
      "Stage 59 JVMHeapMemory maxVal bytes => 1963945440 (1873,0 MB)\n",
      "Stage 59 OnHeapExecutionMemory maxVal bytes => 0 (0 Bytes)\n",
      "Stage 62 JVMHeapMemory maxVal bytes => 1963945440 (1873,0 MB)\n",
      "Stage 62 OnHeapExecutionMemory maxVal bytes => 0 (0 Bytes)\n"
     ]
    }
   ],
   "source": [
    "stagemetrics.print_memory_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
