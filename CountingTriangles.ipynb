{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SetUp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "from os.path import abspath\n",
    "os.environ['SPARK_HOME'] = '/home/chiara/Documenti/BigData/CountingTriangles/spark-3.5.0-bin-hadoop3'\n",
    "os.environ['HADOOP_HOME'] = '/home/chiara/Documenti/BigData/CountingTriangles/spark-3.5.0-bin-hadoop3'\n",
    "os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-17-openjdk-amd64'\n",
    "os.environ['SPARK_LOCAL_IP'] = '172.17.0.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_python = os.path.join(os.environ.get('SPARK_HOME',None),'python')\n",
    "py4j = glob.glob(os.path.join(spark_python,'lib','py4j-*.zip'))[0]\n",
    "graphf = glob.glob(os.path.join(spark_python,'graphframes.zip'))[0]\n",
    "sys.path[:0]=[spark_python,py4j]\n",
    "sys.path[:0]=[spark_python,graphf]\n",
    "os.environ['PYTHONPATH']=py4j+os.pathsep+graphf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/chiara/Documenti/BigData/CountingTriangles/spark-3.5.0-bin-hadoop3'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/02/21 18:32:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Counting Triangles\").enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphframes import *\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset/Wiki-Vote.txt','r') as f:\n",
    "    content = f.readlines()\n",
    "\n",
    "edges_list = list(filter( lambda x: not x.startswith('#') ,content))\n",
    "edges = list(map(lambda x: tuple(x.split('\\t')), edges_list))\n",
    "edges_tuples = list(map(lambda x: (int(x[0]), int(x[1].replace('\\n',''))), edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType, Row\n",
    "\n",
    "list1, list2 = zip(*edges_tuples)\n",
    "nodes = list(set(list1 + list2))\n",
    "nodes_tuple = [Row(x) for x in nodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, collect_list\n",
    "\n",
    "#get list of nodes, with columns renamed value and id\n",
    "vertices = spark.createDataFrame(nodes, IntegerType())\n",
    "vertices = vertices.withColumnRenamed('value','id')\n",
    "\n",
    "#get edges such that the src node is always smaller then the dst node\n",
    "edges_n = spark.createDataFrame(edges_tuples,[\"src\", \"dst\"],IntegerType())\n",
    "edges_inverted = edges_n.filter(edges_n.src > edges_n.dst)\n",
    "edges_normal = edges_n.filter(edges_n.src < edges_n.dst)\n",
    "edges_normal2 = edges_inverted.select(col('dst').alias('src'),col('src').alias('dst'))\n",
    "edges = edges_normal.union(edges_normal2).distinct()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "e1 = edges.alias('e1')\n",
    "e2 = edges.alias('e2')\n",
    "e3 = edges.alias('e3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|src| dst|\n",
      "+---+----+\n",
      "|  3| 586|\n",
      "| 25| 255|\n",
      "| 25| 590|\n",
      "|  7|1193|\n",
      "|  8| 232|\n",
      "|  8| 607|\n",
      "| 11| 958|\n",
      "| 11|1437|\n",
      "| 11|2595|\n",
      "| 19|  61|\n",
      "| 23| 302|\n",
      "| 47|3352|\n",
      "| 29|3717|\n",
      "| 29|4088|\n",
      "| 86|1305|\n",
      "| 99|5812|\n",
      "|103|2871|\n",
      "|108|8283|\n",
      "|109|7052|\n",
      "|122|3258|\n",
      "+---+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "e1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "result = e1.join(e2, col(\"e1.src\") == col(\"e2.src\")) \\\n",
    "    .join(e3, (col(\"e1.dst\") == col(\"e3.src\")) & (col(\"e2.dst\") == col(\"e3.dst\"))) \\\n",
    "    .select(col(\"e1.src\").alias(\"node1\"), col(\"e1.dst\").alias(\"node2\"), col(\"e2.dst\").alias(\"node3\")).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "608389"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[node1#87L, node2#88L, node3#89L], functions=[])\n",
      "   +- HashAggregate(keys=[node1#87L, node2#88L, node3#89L], functions=[])\n",
      "      +- Project [src#4L AS node1#87L, dst#5L AS node2#88L, dst#60L AS node3#89L]\n",
      "         +- SortMergeJoin [dst#5L, dst#60L], [src#71L, dst#72L], Inner\n",
      "            :- Sort [dst#5L ASC NULLS FIRST, dst#60L ASC NULLS FIRST], false, 0\n",
      "            :  +- Exchange hashpartitioning(dst#5L, dst#60L, 200), ENSURE_REQUIREMENTS, [plan_id=11647]\n",
      "            :     +- Project [src#4L, dst#5L, dst#60L]\n",
      "            :        +- SortMergeJoin [src#4L], [src#59L], Inner\n",
      "            :           :- Sort [src#4L ASC NULLS FIRST], false, 0\n",
      "            :           :  +- Exchange hashpartitioning(src#4L, 200), ENSURE_REQUIREMENTS, [plan_id=11637]\n",
      "            :           :     +- HashAggregate(keys=[src#4L, dst#5L], functions=[])\n",
      "            :           :        +- Exchange hashpartitioning(src#4L, dst#5L, 200), ENSURE_REQUIREMENTS, [plan_id=11631]\n",
      "            :           :           +- HashAggregate(keys=[src#4L, dst#5L], functions=[])\n",
      "            :           :              +- Union\n",
      "            :           :                 :- Filter ((isnotnull(src#4L) AND isnotnull(dst#5L)) AND (src#4L < dst#5L))\n",
      "            :           :                 :  +- Scan ExistingRDD[src#4L,dst#5L]\n",
      "            :           :                 +- Project [dst#13L AS src#8L, src#12L AS dst#9L]\n",
      "            :           :                    +- Filter ((isnotnull(src#12L) AND isnotnull(dst#13L)) AND (src#12L > dst#13L))\n",
      "            :           :                       +- Scan ExistingRDD[src#12L,dst#13L]\n",
      "            :           +- Sort [src#59L ASC NULLS FIRST], false, 0\n",
      "            :              +- Exchange hashpartitioning(src#59L, 200), ENSURE_REQUIREMENTS, [plan_id=11638]\n",
      "            :                 +- HashAggregate(keys=[src#59L, dst#60L], functions=[])\n",
      "            :                    +- Exchange hashpartitioning(src#59L, dst#60L, 200), ENSURE_REQUIREMENTS, [plan_id=11633]\n",
      "            :                       +- HashAggregate(keys=[src#59L, dst#60L], functions=[])\n",
      "            :                          +- Union\n",
      "            :                             :- Filter ((isnotnull(src#59L) AND isnotnull(dst#60L)) AND (src#59L < dst#60L))\n",
      "            :                             :  +- Scan ExistingRDD[src#59L,dst#60L]\n",
      "            :                             +- Project [dst#62L AS src#8L, src#61L AS dst#9L]\n",
      "            :                                +- Filter ((isnotnull(src#61L) AND isnotnull(dst#62L)) AND (src#61L > dst#62L))\n",
      "            :                                   +- Scan ExistingRDD[src#61L,dst#62L]\n",
      "            +- Sort [src#71L ASC NULLS FIRST, dst#72L ASC NULLS FIRST], false, 0\n",
      "               +- HashAggregate(keys=[src#71L, dst#72L], functions=[])\n",
      "                  +- Exchange hashpartitioning(src#71L, dst#72L, 200), ENSURE_REQUIREMENTS, [plan_id=11643]\n",
      "                     +- HashAggregate(keys=[src#71L, dst#72L], functions=[])\n",
      "                        +- Union\n",
      "                           :- Filter ((isnotnull(src#71L) AND isnotnull(dst#72L)) AND (src#71L < dst#72L))\n",
      "                           :  +- Scan ExistingRDD[src#71L,dst#72L]\n",
      "                           +- Project [dst#74L AS src#8L, src#73L AS dst#9L]\n",
      "                              +- Filter ((isnotnull(src#73L) AND isnotnull(dst#74L)) AND (src#73L > dst#74L))\n",
      "                                 +- Scan ExistingRDD[src#73L,dst#74L]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confront with another implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = GraphFrame(vertices,edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "triangles = graph.triangleCount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "|count| id|\n",
      "+-----+---+\n",
      "|  197| 12|\n",
      "|    0| 22|\n",
      "|   12| 13|\n",
      "| 3143|  6|\n",
      "|   23| 16|\n",
      "|  280|  3|\n",
      "| 1078| 20|\n",
      "|   88|  5|\n",
      "|  482| 19|\n",
      "| 4847| 15|\n",
      "|  321|  9|\n",
      "|  156| 17|\n",
      "|   95|  4|\n",
      "| 2294|  8|\n",
      "|  661| 23|\n",
      "|   42|  7|\n",
      "|  700| 10|\n",
      "|  164| 21|\n",
      "|13401| 11|\n",
      "|  361| 14|\n",
      "+-----+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "triangles.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 108:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "| count|\n",
      "+------+\n",
      "|608389|\n",
      "+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import sum\n",
    "\n",
    "triangle_count = triangles.select(sum(\"count\")/3)\n",
    "triangle_count.select(col('(sum(count) / 3)').cast(IntegerType()).alias('count')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confront with another algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_good_line(line):\n",
    "    # filter lines in the input file\n",
    "    try:\n",
    "        fields = line.split('\\t')\n",
    "        if len(fields) > 2:\n",
    "            return False\n",
    "\n",
    "        float(fields[-1])\n",
    "        return True\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def generate_combinations(line):\n",
    "    # create all combinations and return key-value pairs in format ((v1,v2), ['to_check'])\n",
    "    l = line[1]\n",
    "    return [(pair, ['to_check']) for pair in itertools.combinations(l, 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def degree_filter(t):\n",
    "    # not considering vertices with degree less than 2\n",
    "    # as there are no possible wedges/closed wedges which they could form\n",
    "    if len(t[1]) < 2:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "lines = sc.textFile(\"./dataset/Wiki-Vote.txt\")\n",
    "rdd_edges = lines.filter(is_good_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will check for edges only, so create key-value pairs ((v1, v2), 'exists')\n",
    "existing_edges_1 = rdd_edges.map(lambda l: ((l.split('\\t')[0], l.split('\\t')[1]), ['exists']))\n",
    "existing_edges_2 = rdd_edges.map(lambda l: ((l.split('\\t')[1], l.split('\\t')[0]), ['exists']))\n",
    "existing_edges = existing_edges_1.union(existing_edges_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('30', '1412'), ['exists']),\n",
       " (('30', '3352'), ['exists']),\n",
       " (('30', '5254'), ['exists']),\n",
       " (('30', '5543'), ['exists']),\n",
       " (('30', '7478'), ['exists'])]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "existing_edges.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_1 = rdd_edges.map(lambda l: (l.split('\\t')[0], l.split('\\t')[1]))\n",
    "edges_2 = rdd_edges.map(lambda l: (l.split('\\t')[0], l.split('\\t')[1]))\n",
    "\n",
    "edges = edges_1.union(edges_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create rdd that has key-value pairs\n",
    "# (vertex1, [nb1,nb2,nb3...]), where nbi is neighbour i of vertex1\n",
    "neighbours = edges.groupByKey().map(lambda x : (x[0], list(set(list(x[1])))))\n",
    "neighbours = neighbours.filter(degree_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate all pairs ((v1, v2), ['to_check'])\n",
    "# use flatMap as generate_combinations returns lists\n",
    "edges_to_check = neighbours.map(generate_combinations).flatMap(lambda x: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform summation over lists containing 'to_check'\n",
    "edges_to_check = edges_to_check.reduceByKey(lambda a,b: a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create rdd in the format ((v1, v2), ('exists', ['to_check', 'to_check',...]))\n",
    "all_edges = existing_edges.join(edges_to_check)\n",
    "all_edges = all_edges.map(lambda x: (x[0], len(x[1][1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate count for each vertex\n",
    "vertices_part1 = all_edges.map(lambda y: (y[0][0], y[1]))\n",
    "vertices_part2 = all_edges.map(lambda y: (y[0][1], y[1]))\n",
    "vertex_pair = vertices_part1.union(vertices_part2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summing over all vertex counts\n",
    "vertex = vertex_pair.reduceByKey(lambda a,b: a+b)\n",
    "final = vertex.map(lambda x: (x[0], x[1]/6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from operator import add\n",
    "triangle_count2 = final.map(lambda x: x[1]).reduce(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "248852.33333333337"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triangle_count2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
