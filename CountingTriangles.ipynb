{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SetUp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "from os.path import abspath\n",
    "os.environ['SPARK_HOME'] = '/home/chiara/Documenti/BigData/CountingTriangles/spark-3.5.0-bin-hadoop3'\n",
    "os.environ['HADOOP_HOME'] = '/home/chiara/Documenti/BigData/CountingTriangles/spark-3.5.0-bin-hadoop3'\n",
    "os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-17-openjdk-amd64'\n",
    "os.environ['SPARK_LOCAL_IP'] = '172.17.0.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_python = os.path.join(os.environ.get('SPARK_HOME',None),'python')\n",
    "py4j = glob.glob(os.path.join(spark_python,'lib','py4j-*.zip'))[0]\n",
    "graphf = glob.glob(os.path.join(spark_python,'graphframes.zip'))[0]\n",
    "sparkmeasure = glob.glob(os.path.join(spark_python,'sparkmeasure.zip'))[0]\n",
    "sys.path[:0]=[spark_python,py4j]\n",
    "sys.path[:0]=[spark_python,graphf]\n",
    "sys.path[:0]=[spark_python, sparkmeasure]\n",
    "os.environ['PYTHONPATH']=py4j+os.pathsep+graphf+sparkmeasure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/chiara/Documenti/BigData/CountingTriangles/spark-3.5.0-bin-hadoop3'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.sql import SparkSession\n",
    "\n",
    "#spark = SparkSession.builder.appName(\"Counting Triangles\").set(\"spark.driver.memory\", \"8g\").enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/02/23 10:33:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "    .appName('Counting Triangles')\n",
    "    .config('spark.driver.extraClassPath', '/usr/local/bin/postgresql-42.2.5.jar')\n",
    "    .config('spark.executor.memory', '8g')\n",
    "    .config(\"spark.driver.memory\", \"8g\")\n",
    "    .config('spark.memory.offHeap.enabled', True)\n",
    "    .config('spark.memory.offHeap.size', '20g') \n",
    "    .enableHiveSupport()\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphframes import *\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'datasets/soc-Epinions1.txt'\n",
    "with open(dataset,'r') as f:\n",
    "    content = f.readlines()\n",
    "edges_list = list(filter( lambda x: not x.startswith('#') ,content))\n",
    "\n",
    "\n",
    "if 'facebook' in dataset:\n",
    "    edges = list(map(lambda x: tuple(x.split(' ')), edges_list))\n",
    "else:\n",
    "    edges = list(map(lambda x: tuple(x.split('\\t')), edges_list))\n",
    "\n",
    "edges_tuples = list(map(lambda x: (int(x[0]), int(x[1].replace('\\n',''))), edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType, Row\n",
    "\n",
    "list1, list2 = zip(*edges_tuples)\n",
    "nodes = list(set(list1 + list2))\n",
    "nodes_tuple = [Row(x) for x in nodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, collect_list\n",
    "\n",
    "#get list of nodes, with columns renamed value and id\n",
    "vertices = spark.createDataFrame(nodes, IntegerType())\n",
    "vertices = vertices.withColumnRenamed('value','id')\n",
    "\n",
    "#get edges such that the src node is always smaller then the dst node\n",
    "edges_n = spark.createDataFrame(edges_tuples,[\"src\", \"dst\"],IntegerType())\n",
    "edges_inverted = edges_n.filter(edges_n.src > edges_n.dst)\n",
    "edges_normal = edges_n.filter(edges_n.src < edges_n.dst)\n",
    "edges_normal2 = edges_inverted.select(col('dst').alias('src'),col('src').alias('dst'))\n",
    "edges = edges_normal.union(edges_normal2).distinct()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check we read the right data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "75879"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vertices.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For dataset soc-Epinions1 the found edges are not the same length as referred in the dataset page, this is because of the distinct() operation at the end of the selection of edges: we treat each graph as undirected and only consider edges where srcId < dstId, so when we encounter two edges of the kind (srcId, dstId) (dstId, srcId), with srcId < dstId or viceversa, we're going to keep just one edge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "405740"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "e1 = edges.alias('e1')\n",
    "e2 = edges.alias('e2')\n",
    "e3 = edges.alias('e3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|src| dst|\n",
      "+---+----+\n",
      "|  4|  19|\n",
      "|  5| 443|\n",
      "| 11|  97|\n",
      "| 11| 329|\n",
      "| 12| 382|\n",
      "| 12| 386|\n",
      "| 12| 780|\n",
      "| 15|  18|\n",
      "| 17|1777|\n",
      "| 19|2030|\n",
      "| 22|  25|\n",
      "| 25|2461|\n",
      "| 28| 130|\n",
      "| 28|1512|\n",
      "| 28|2198|\n",
      "| 31|1343|\n",
      "| 31|2491|\n",
      "| 34| 185|\n",
      "| 34|1171|\n",
      "| 34|1370|\n",
      "+---+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "e1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparkmeasure import StageMetrics\n",
    "\n",
    "stagemetrics = StageMetrics(spark)\n",
    "stagemetrics.begin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "result = e1.join(e2, col(\"e1.src\") == col(\"e2.src\")) \\\n",
    "    .join(e3, (col(\"e1.dst\") == col(\"e3.src\")) & (col(\"e2.dst\") == col(\"e3.dst\"))) \\\n",
    "    .select(col(\"e1.src\").alias(\"node1\"), col(\"e1.dst\").alias(\"node2\"), col(\"e2.dst\").alias(\"node3\")).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1624481"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "stagemetrics.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[node1#66L, node2#67L, node3#68L], functions=[])\n",
      "   +- HashAggregate(keys=[node1#66L, node2#67L, node3#68L], functions=[])\n",
      "      +- Project [src#4L AS node1#66L, dst#5L AS node2#67L, dst#39L AS node3#68L]\n",
      "         +- SortMergeJoin [dst#5L, dst#39L], [src#50L, dst#51L], Inner\n",
      "            :- Sort [dst#5L ASC NULLS FIRST, dst#39L ASC NULLS FIRST], false, 0\n",
      "            :  +- Exchange hashpartitioning(dst#5L, dst#39L, 200), ENSURE_REQUIREMENTS, [plan_id=1168]\n",
      "            :     +- Project [src#4L, dst#5L, dst#39L]\n",
      "            :        +- SortMergeJoin [src#4L], [src#38L], Inner\n",
      "            :           :- Sort [src#4L ASC NULLS FIRST], false, 0\n",
      "            :           :  +- Exchange hashpartitioning(src#4L, 200), ENSURE_REQUIREMENTS, [plan_id=1158]\n",
      "            :           :     +- HashAggregate(keys=[src#4L, dst#5L], functions=[])\n",
      "            :           :        +- Exchange hashpartitioning(src#4L, dst#5L, 200), ENSURE_REQUIREMENTS, [plan_id=1152]\n",
      "            :           :           +- HashAggregate(keys=[src#4L, dst#5L], functions=[])\n",
      "            :           :              +- Union\n",
      "            :           :                 :- Filter ((isnotnull(src#4L) AND isnotnull(dst#5L)) AND (src#4L < dst#5L))\n",
      "            :           :                 :  +- Scan ExistingRDD[src#4L,dst#5L]\n",
      "            :           :                 +- Project [dst#13L AS src#8L, src#12L AS dst#9L]\n",
      "            :           :                    +- Filter ((isnotnull(src#12L) AND isnotnull(dst#13L)) AND (src#12L > dst#13L))\n",
      "            :           :                       +- Scan ExistingRDD[src#12L,dst#13L]\n",
      "            :           +- Sort [src#38L ASC NULLS FIRST], false, 0\n",
      "            :              +- Exchange hashpartitioning(src#38L, 200), ENSURE_REQUIREMENTS, [plan_id=1159]\n",
      "            :                 +- HashAggregate(keys=[src#38L, dst#39L], functions=[])\n",
      "            :                    +- Exchange hashpartitioning(src#38L, dst#39L, 200), ENSURE_REQUIREMENTS, [plan_id=1154]\n",
      "            :                       +- HashAggregate(keys=[src#38L, dst#39L], functions=[])\n",
      "            :                          +- Union\n",
      "            :                             :- Filter ((isnotnull(src#38L) AND isnotnull(dst#39L)) AND (src#38L < dst#39L))\n",
      "            :                             :  +- Scan ExistingRDD[src#38L,dst#39L]\n",
      "            :                             +- Project [dst#41L AS src#8L, src#40L AS dst#9L]\n",
      "            :                                +- Filter ((isnotnull(src#40L) AND isnotnull(dst#41L)) AND (src#40L > dst#41L))\n",
      "            :                                   +- Scan ExistingRDD[src#40L,dst#41L]\n",
      "            +- Sort [src#50L ASC NULLS FIRST, dst#51L ASC NULLS FIRST], false, 0\n",
      "               +- HashAggregate(keys=[src#50L, dst#51L], functions=[])\n",
      "                  +- Exchange hashpartitioning(src#50L, dst#51L, 200), ENSURE_REQUIREMENTS, [plan_id=1164]\n",
      "                     +- HashAggregate(keys=[src#50L, dst#51L], functions=[])\n",
      "                        +- Union\n",
      "                           :- Filter ((isnotnull(src#50L) AND isnotnull(dst#51L)) AND (src#50L < dst#51L))\n",
      "                           :  +- Scan ExistingRDD[src#50L,dst#51L]\n",
      "                           +- Project [dst#53L AS src#8L, src#52L AS dst#9L]\n",
      "                              +- Filter ((isnotnull(src#52L) AND isnotnull(dst#53L)) AND (src#52L > dst#53L))\n",
      "                                 +- Scan ExistingRDD[src#52L,dst#53L]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure the performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scheduling mode = FIFO\n",
      "Spark Context default degree of parallelism = 12\n",
      "\n",
      "Aggregated Spark stage metrics:\n",
      "numStages => 6\n",
      "numTasks => 53\n",
      "elapsedTime => 41589 (42 s)\n",
      "stageDuration => 40958 (41 s)\n",
      "executorRunTime => 252319 (4,2 min)\n",
      "executorCpuTime => 232442 (3,9 min)\n",
      "executorDeserializeTime => 413 (0,4 s)\n",
      "executorDeserializeCpuTime => 280 (0,3 s)\n",
      "resultSerializationTime => 4 (4 ms)\n",
      "jvmGCTime => 838 (0,8 s)\n",
      "shuffleFetchWaitTime => 0 (0 ms)\n",
      "shuffleWriteTime => 4124 (4 s)\n",
      "resultSize => 3694599 (3,5 MB)\n",
      "diskBytesSpilled => 0 (0 Bytes)\n",
      "memoryBytesSpilled => 0 (0 Bytes)\n",
      "peakExecutionMemory => 14706526584\n",
      "recordsRead => 0\n",
      "bytesRead => 0 (0 Bytes)\n",
      "recordsWritten => 0\n",
      "bytesWritten => 0 (0 Bytes)\n",
      "shuffleRecordsRead => 91283228\n",
      "shuffleTotalBlocksFetched => 604\n",
      "shuffleLocalBlocksFetched => 604\n",
      "shuffleRemoteBlocksFetched => 0\n",
      "shuffleTotalBytesRead => 936325617 (892,9 MB)\n",
      "shuffleLocalBytesRead => 936325617 (892,9 MB)\n",
      "shuffleRemoteBytesRead => 0 (0 Bytes)\n",
      "shuffleRemoteBytesReadToDisk => 0 (0 Bytes)\n",
      "shuffleBytesWritten => 926068206 (883,2 MB)\n",
      "shuffleRecordsWritten => 90368651\n",
      "\n",
      "Stages and their duration:\n",
      "Stage 12 duration => 1091 (1 s)\n",
      "Stage 14 duration => 276 (0,3 s)\n",
      "Stage 17 duration => 185 (0,2 s)\n",
      "Stage 20 duration => 19071 (19 s)\n",
      "Stage 24 duration => 20320 (20 s)\n",
      "Stage 29 duration => 15 (15 ms)\n"
     ]
    }
   ],
   "source": [
    "stagemetrics.print_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Additional stage-level executor metrics (memory usage info):\n",
      "\n",
      "Stage 12 JVMHeapMemory maxVal bytes => 1006347088 (959,7 MB)\n",
      "Stage 12 OnHeapExecutionMemory maxVal bytes => 0 (0 Bytes)\n",
      "Stage 14 JVMHeapMemory maxVal bytes => 1006347088 (959,7 MB)\n",
      "Stage 14 OnHeapExecutionMemory maxVal bytes => 0 (0 Bytes)\n",
      "Stage 17 JVMHeapMemory maxVal bytes => 1006347088 (959,7 MB)\n",
      "Stage 17 OnHeapExecutionMemory maxVal bytes => 0 (0 Bytes)\n",
      "Stage 20 JVMHeapMemory maxVal bytes => 1113126536 (1061,6 MB)\n",
      "Stage 20 OnHeapExecutionMemory maxVal bytes => 0 (0 Bytes)\n",
      "Stage 24 JVMHeapMemory maxVal bytes => 1113126536 (1061,6 MB)\n",
      "Stage 24 OnHeapExecutionMemory maxVal bytes => 0 (0 Bytes)\n",
      "Stage 29 JVMHeapMemory maxVal bytes => 565638064 (539,4 MB)\n",
      "Stage 29 OnHeapExecutionMemory maxVal bytes => 0 (0 Bytes)\n"
     ]
    }
   ],
   "source": [
    "stagemetrics.print_memory_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confront with another implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = GraphFrame(vertices,edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "stagemetrics = StageMetrics(spark)\n",
    "stagemetrics.begin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "triangles = graph.triangleCount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 50:===================================================>    (12 + 1) / 13]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "|count| id|\n",
      "+-----+---+\n",
      "|10229| 12|\n",
      "|16511|  1|\n",
      "| 2572| 13|\n",
      "| 1971|  6|\n",
      "|  969| 16|\n",
      "|  302|  3|\n",
      "| 2260| 20|\n",
      "| 4854|  5|\n",
      "|10770| 19|\n",
      "|  238| 15|\n",
      "|   83|  9|\n",
      "|  234| 17|\n",
      "| 2716|  4|\n",
      "|  612|  8|\n",
      "|  107|  7|\n",
      "| 5865| 10|\n",
      "| 5729| 11|\n",
      "| 1222| 14|\n",
      "| 3885|  2|\n",
      "|12462|  0|\n",
      "+-----+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "triangles.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 80:===================================================>    (12 + 1) / 13]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|  count|\n",
      "+-------+\n",
      "|1624481|\n",
      "+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import sum\n",
    "\n",
    "triangle_count = triangles.select(sum(\"count\")/3)\n",
    "triangle_count.select(col('(sum(count) / 3)').cast(IntegerType()).alias('count')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "stagemetrics.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [CASE WHEN isnull(count#180L) THEN 0 ELSE count#180L END AS count#190L, id#2]\n",
      "   +- SortMergeJoin [id#2], [id#176], LeftOuter\n",
      "      :- Sort [id#2 ASC NULLS FIRST], false, 0\n",
      "      :  +- Exchange hashpartitioning(id#2, 200), ENSURE_REQUIREMENTS, [plan_id=5573]\n",
      "      :     +- Project [value#0 AS id#2]\n",
      "      :        +- Scan ExistingRDD[value#0]\n",
      "      +- Sort [id#176 ASC NULLS FIRST], false, 0\n",
      "         +- HashAggregate(keys=[id#176], functions=[count(1)])\n",
      "            +- Exchange hashpartitioning(id#176, 200), ENSURE_REQUIREMENTS, [plan_id=5569]\n",
      "               +- HashAggregate(keys=[id#176], functions=[partial_count(1)])\n",
      "                  +- Filter isnotnull(id#176)\n",
      "                     +- Generate explode(array(a#97.id, b#99.id, c#120.id)), false, [id#176]\n",
      "                        +- Project [a#97, b#99, c#120]\n",
      "                           +- SortMergeJoin [cast(a#97.id as bigint), cast(c#120.id as bigint)], [__tmp-1043886091038848698#148.src, __tmp-1043886091038848698#148.dst], Inner\n",
      "                              :- Sort [cast(a#97.id as bigint) ASC NULLS FIRST, cast(c#120.id as bigint) ASC NULLS FIRST], false, 0\n",
      "                              :  +- Exchange hashpartitioning(cast(a#97.id as bigint), cast(c#120.id as bigint), 200), ENSURE_REQUIREMENTS, [plan_id=5560]\n",
      "                              :     +- Project [a#97, b#99, c#120]\n",
      "                              :        +- SortMergeJoin [_extract_dst#241L], [cast(c#120.id as bigint)], Inner\n",
      "                              :           :- Sort [_extract_dst#241L ASC NULLS FIRST], false, 0\n",
      "                              :           :  +- Exchange hashpartitioning(_extract_dst#241L, 200), ENSURE_REQUIREMENTS, [plan_id=5549]\n",
      "                              :           :     +- Project [a#97, b#99, __tmp3640351034883199571#118.dst AS _extract_dst#241L]\n",
      "                              :           :        +- SortMergeJoin [cast(b#99.id as bigint)], [__tmp3640351034883199571#118.src], Inner\n",
      "                              :           :           :- Project [a#97, b#99]\n",
      "                              :           :           :  +- SortMergeJoin [_extract_dst#242L], [cast(b#99.id as bigint)], Inner\n",
      "                              :           :           :     :- Sort [_extract_dst#242L ASC NULLS FIRST], false, 0\n",
      "                              :           :           :     :  +- Exchange hashpartitioning(_extract_dst#242L, 200), ENSURE_REQUIREMENTS, [plan_id=5533]\n",
      "                              :           :           :     :     +- Project [__tmp-4363599943432734077#95.dst AS _extract_dst#242L, a#97]\n",
      "                              :           :           :     :        +- SortMergeJoin [__tmp-4363599943432734077#95.src], [cast(a#97.id as bigint)], Inner\n",
      "                              :           :           :     :           :- Sort [__tmp-4363599943432734077#95.src ASC NULLS FIRST], false, 0\n",
      "                              :           :           :     :           :  +- Exchange hashpartitioning(__tmp-4363599943432734077#95.src, 200), ENSURE_REQUIREMENTS, [plan_id=5525]\n",
      "                              :           :           :     :           :     +- HashAggregate(keys=[src#91L, dst#92L], functions=[])\n",
      "                              :           :           :     :           :        +- Exchange hashpartitioning(src#91L, dst#92L, 200), ENSURE_REQUIREMENTS, [plan_id=5521]\n",
      "                              :           :           :     :           :           +- HashAggregate(keys=[src#91L, dst#92L], functions=[])\n",
      "                              :           :           :     :           :              +- Union\n",
      "                              :           :           :     :           :                 :- Project [if ((src#4L < dst#5L)) src#4L else dst#5L AS src#91L, if ((src#4L < dst#5L)) dst#5L else src#4L AS dst#92L]\n",
      "                              :           :           :     :           :                 :  +- Filter (((isnotnull(src#4L) AND isnotnull(dst#5L)) AND ((src#4L < dst#5L) AND NOT (src#4L = dst#5L))) AND (isnotnull(if ((src#4L < dst#5L)) src#4L else dst#5L) AND isnotnull(if ((src#4L < dst#5L)) dst#5L else src#4L)))\n",
      "                              :           :           :     :           :                 :     +- Scan ExistingRDD[src#4L,dst#5L]\n",
      "                              :           :           :     :           :                 +- Project [if ((dst#13L < src#12L)) dst#13L else src#12L AS src#243L, if ((dst#13L < src#12L)) src#12L else dst#13L AS dst#244L]\n",
      "                              :           :           :     :           :                    +- Filter (((isnotnull(src#12L) AND isnotnull(dst#13L)) AND ((src#12L > dst#13L) AND NOT (dst#13L = src#12L))) AND (isnotnull(if ((dst#13L < src#12L)) dst#13L else src#12L) AND isnotnull(if ((dst#13L < src#12L)) src#12L else dst#13L)))\n",
      "                              :           :           :     :           :                       +- Scan ExistingRDD[src#12L,dst#13L]\n",
      "                              :           :           :     :           +- Sort [cast(a#97.id as bigint) ASC NULLS FIRST], false, 0\n",
      "                              :           :           :     :              +- Exchange hashpartitioning(cast(a#97.id as bigint), 200), ENSURE_REQUIREMENTS, [plan_id=5526]\n",
      "                              :           :           :     :                 +- Project [struct(id, value#183) AS a#97]\n",
      "                              :           :           :     :                    +- Filter isnotnull(value#183)\n",
      "                              :           :           :     :                       +- Scan ExistingRDD[value#183]\n",
      "                              :           :           :     +- Sort [cast(b#99.id as bigint) ASC NULLS FIRST], false, 0\n",
      "                              :           :           :        +- Exchange hashpartitioning(cast(b#99.id as bigint), 200), ENSURE_REQUIREMENTS, [plan_id=5534]\n",
      "                              :           :           :           +- Project [struct(id, value#109) AS b#99]\n",
      "                              :           :           :              +- Filter isnotnull(value#109)\n",
      "                              :           :           :                 +- Scan ExistingRDD[value#109]\n",
      "                              :           :           +- Sort [__tmp3640351034883199571#118.src ASC NULLS FIRST], false, 0\n",
      "                              :           :              +- Exchange hashpartitioning(__tmp3640351034883199571#118.src, 200), ENSURE_REQUIREMENTS, [plan_id=5543]\n",
      "                              :           :                 +- HashAggregate(keys=[src#91L, dst#92L], functions=[])\n",
      "                              :           :                    +- Exchange hashpartitioning(src#91L, dst#92L, 200), ENSURE_REQUIREMENTS, [plan_id=5539]\n",
      "                              :           :                       +- HashAggregate(keys=[src#91L, dst#92L], functions=[])\n",
      "                              :           :                          +- Union\n",
      "                              :           :                             :- Project [if ((src#124L < dst#125L)) src#124L else dst#125L AS src#91L, if ((src#124L < dst#125L)) dst#125L else src#124L AS dst#92L]\n",
      "                              :           :                             :  +- Filter (((isnotnull(src#124L) AND isnotnull(dst#125L)) AND ((src#124L < dst#125L) AND NOT (src#124L = dst#125L))) AND (isnotnull(if ((src#124L < dst#125L)) src#124L else dst#125L) AND isnotnull(if ((src#124L < dst#125L)) dst#125L else src#124L)))\n",
      "                              :           :                             :     +- Scan ExistingRDD[src#124L,dst#125L]\n",
      "                              :           :                             +- Project [if ((dst#127L < src#126L)) dst#127L else src#126L AS src#245L, if ((dst#127L < src#126L)) src#126L else dst#127L AS dst#246L]\n",
      "                              :           :                                +- Filter (((isnotnull(src#126L) AND isnotnull(dst#127L)) AND ((src#126L > dst#127L) AND NOT (dst#127L = src#126L))) AND (isnotnull(if ((dst#127L < src#126L)) dst#127L else src#126L) AND isnotnull(if ((dst#127L < src#126L)) src#126L else dst#127L)))\n",
      "                              :           :                                   +- Scan ExistingRDD[src#126L,dst#127L]\n",
      "                              :           +- Sort [cast(c#120.id as bigint) ASC NULLS FIRST], false, 0\n",
      "                              :              +- Exchange hashpartitioning(cast(c#120.id as bigint), 200), ENSURE_REQUIREMENTS, [plan_id=5550]\n",
      "                              :                 +- Project [struct(id, value#136) AS c#120]\n",
      "                              :                    +- Filter isnotnull(value#136)\n",
      "                              :                       +- Scan ExistingRDD[value#136]\n",
      "                              +- Sort [__tmp-1043886091038848698#148.src ASC NULLS FIRST, __tmp-1043886091038848698#148.dst ASC NULLS FIRST], false, 0\n",
      "                                 +- Exchange hashpartitioning(__tmp-1043886091038848698#148.src, __tmp-1043886091038848698#148.dst, 200), ENSURE_REQUIREMENTS, [plan_id=5559]\n",
      "                                    +- HashAggregate(keys=[src#91L, dst#92L], functions=[])\n",
      "                                       +- Exchange hashpartitioning(src#91L, dst#92L, 200), ENSURE_REQUIREMENTS, [plan_id=5555]\n",
      "                                          +- HashAggregate(keys=[src#91L, dst#92L], functions=[])\n",
      "                                             +- Union\n",
      "                                                :- Project [if ((src#154L < dst#155L)) src#154L else dst#155L AS src#91L, if ((src#154L < dst#155L)) dst#155L else src#154L AS dst#92L]\n",
      "                                                :  +- Filter (((isnotnull(src#154L) AND isnotnull(dst#155L)) AND ((src#154L < dst#155L) AND NOT (src#154L = dst#155L))) AND (isnotnull(if ((src#154L < dst#155L)) src#154L else dst#155L) AND isnotnull(if ((src#154L < dst#155L)) dst#155L else src#154L)))\n",
      "                                                :     +- Scan ExistingRDD[src#154L,dst#155L]\n",
      "                                                +- Project [if ((dst#157L < src#156L)) dst#157L else src#156L AS src#247L, if ((dst#157L < src#156L)) src#156L else dst#157L AS dst#248L]\n",
      "                                                   +- Filter (((isnotnull(src#156L) AND isnotnull(dst#157L)) AND ((src#156L > dst#157L) AND NOT (dst#157L = src#156L))) AND (isnotnull(if ((dst#157L < src#156L)) dst#157L else src#156L) AND isnotnull(if ((dst#157L < src#156L)) src#156L else dst#157L)))\n",
      "                                                      +- Scan ExistingRDD[src#156L,dst#157L]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "triangles.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scheduling mode = FIFO\n",
      "Spark Context default degree of parallelism = 12\n",
      "\n",
      "Aggregated Spark stage metrics:\n",
      "numStages => 23\n",
      "numTasks => 177\n",
      "elapsedTime => 26516 (27 s)\n",
      "stageDuration => 31733 (32 s)\n",
      "executorRunTime => 239041 (4,0 min)\n",
      "executorCpuTime => 173399 (2,9 min)\n",
      "executorDeserializeTime => 955 (1,0 s)\n",
      "executorDeserializeCpuTime => 611 (0,6 s)\n",
      "resultSerializationTime => 34 (34 ms)\n",
      "jvmGCTime => 1993 (2 s)\n",
      "shuffleFetchWaitTime => 0 (0 ms)\n",
      "shuffleWriteTime => 11638 (12 s)\n",
      "resultSize => 395143 (385,9 KB)\n",
      "diskBytesSpilled => 0 (0 Bytes)\n",
      "memoryBytesSpilled => 0 (0 Bytes)\n",
      "peakExecutionMemory => 15923869408\n",
      "recordsRead => 0\n",
      "bytesRead => 0 (0 Bytes)\n",
      "recordsWritten => 0\n",
      "bytesWritten => 0 (0 Bytes)\n",
      "shuffleRecordsRead => 49117580\n",
      "shuffleTotalBlocksFetched => 1053\n",
      "shuffleLocalBlocksFetched => 1053\n",
      "shuffleRemoteBlocksFetched => 0\n",
      "shuffleTotalBytesRead => 577715523 (551,0 MB)\n",
      "shuffleLocalBytesRead => 577715523 (551,0 MB)\n",
      "shuffleRemoteBytesRead => 0 (0 Bytes)\n",
      "shuffleRemoteBytesReadToDisk => 0 (0 Bytes)\n",
      "shuffleBytesWritten => 556798618 (531,0 MB)\n",
      "shuffleRecordsWritten => 47288678\n",
      "\n",
      "Stages and their duration:\n",
      "Stage 30 duration => 609 (0,6 s)\n",
      "Stage 31 duration => 1768 (2 s)\n",
      "Stage 32 duration => 1917 (2 s)\n",
      "Stage 34 duration => 486 (0,5 s)\n",
      "Stage 35 duration => 428 (0,4 s)\n",
      "Stage 37 duration => 293 (0,3 s)\n",
      "Stage 40 duration => 249 (0,2 s)\n",
      "Stage 44 duration => 2718 (3 s)\n",
      "Stage 50 duration => 7536 (8 s)\n",
      "Stage 57 duration => 124 (0,1 s)\n",
      "Stage 59 duration => 20 (20 ms)\n",
      "Stage 60 duration => 659 (0,7 s)\n",
      "Stage 61 duration => 1889 (2 s)\n",
      "Stage 62 duration => 2239 (2 s)\n",
      "Stage 64 duration => 430 (0,4 s)\n",
      "Stage 65 duration => 419 (0,4 s)\n",
      "Stage 67 duration => 72 (72 ms)\n",
      "Stage 70 duration => 204 (0,2 s)\n",
      "Stage 74 duration => 2795 (3 s)\n",
      "Stage 80 duration => 6764 (7 s)\n",
      "Stage 87 duration => 61 (61 ms)\n",
      "Stage 89 duration => 42 (42 ms)\n",
      "Stage 92 duration => 11 (11 ms)\n"
     ]
    }
   ],
   "source": [
    "stagemetrics.print_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Additional stage-level executor metrics (memory usage info):\n",
      "\n",
      "Stage 30 JVMHeapMemory maxVal bytes => 1018885408 (971,7 MB)\n",
      "Stage 30 OnHeapExecutionMemory maxVal bytes => 0 (0 Bytes)\n",
      "Stage 31 JVMHeapMemory maxVal bytes => 1018885408 (971,7 MB)\n",
      "Stage 31 OnHeapExecutionMemory maxVal bytes => 0 (0 Bytes)\n",
      "Stage 32 JVMHeapMemory maxVal bytes => 1018885408 (971,7 MB)\n",
      "Stage 32 OnHeapExecutionMemory maxVal bytes => 0 (0 Bytes)\n",
      "Stage 34 JVMHeapMemory maxVal bytes => 1018885408 (971,7 MB)\n",
      "Stage 34 OnHeapExecutionMemory maxVal bytes => 0 (0 Bytes)\n",
      "Stage 35 JVMHeapMemory maxVal bytes => 1018885408 (971,7 MB)\n",
      "Stage 35 OnHeapExecutionMemory maxVal bytes => 0 (0 Bytes)\n",
      "Stage 37 JVMHeapMemory maxVal bytes => 1018885408 (971,7 MB)\n",
      "Stage 37 OnHeapExecutionMemory maxVal bytes => 0 (0 Bytes)\n",
      "Stage 40 JVMHeapMemory maxVal bytes => 1018885408 (971,7 MB)\n",
      "Stage 40 OnHeapExecutionMemory maxVal bytes => 0 (0 Bytes)\n",
      "Stage 44 JVMHeapMemory maxVal bytes => 1092642840 (1042,0 MB)\n",
      "Stage 44 OnHeapExecutionMemory maxVal bytes => 0 (0 Bytes)\n",
      "Stage 50 JVMHeapMemory maxVal bytes => 1092642840 (1042,0 MB)\n",
      "Stage 50 OnHeapExecutionMemory maxVal bytes => 0 (0 Bytes)\n",
      "Stage 57 JVMHeapMemory maxVal bytes => 1092642840 (1042,0 MB)\n",
      "Stage 57 OnHeapExecutionMemory maxVal bytes => 0 (0 Bytes)\n",
      "Stage 59 JVMHeapMemory maxVal bytes => 1092642840 (1042,0 MB)\n",
      "Stage 59 OnHeapExecutionMemory maxVal bytes => 0 (0 Bytes)\n",
      "Stage 60 JVMHeapMemory maxVal bytes => 1092642840 (1042,0 MB)\n",
      "Stage 60 OnHeapExecutionMemory maxVal bytes => 0 (0 Bytes)\n",
      "Stage 61 JVMHeapMemory maxVal bytes => 1092642840 (1042,0 MB)\n",
      "Stage 61 OnHeapExecutionMemory maxVal bytes => 0 (0 Bytes)\n",
      "Stage 62 JVMHeapMemory maxVal bytes => 651958160 (621,8 MB)\n",
      "Stage 62 OnHeapExecutionMemory maxVal bytes => 0 (0 Bytes)\n",
      "Stage 64 JVMHeapMemory maxVal bytes => 651958160 (621,8 MB)\n",
      "Stage 64 OnHeapExecutionMemory maxVal bytes => 0 (0 Bytes)\n",
      "Stage 65 JVMHeapMemory maxVal bytes => 651958160 (621,8 MB)\n",
      "Stage 65 OnHeapExecutionMemory maxVal bytes => 0 (0 Bytes)\n",
      "Stage 67 JVMHeapMemory maxVal bytes => 651958160 (621,8 MB)\n",
      "Stage 67 OnHeapExecutionMemory maxVal bytes => 0 (0 Bytes)\n",
      "Stage 70 JVMHeapMemory maxVal bytes => 651958160 (621,8 MB)\n",
      "Stage 70 OnHeapExecutionMemory maxVal bytes => 0 (0 Bytes)\n",
      "Stage 74 JVMHeapMemory maxVal bytes => 651958160 (621,8 MB)\n",
      "Stage 74 OnHeapExecutionMemory maxVal bytes => 0 (0 Bytes)\n",
      "Stage 80 JVMHeapMemory maxVal bytes => 972317960 (927,3 MB)\n",
      "Stage 80 OnHeapExecutionMemory maxVal bytes => 0 (0 Bytes)\n",
      "Stage 87 JVMHeapMemory maxVal bytes => 972317960 (927,3 MB)\n",
      "Stage 87 OnHeapExecutionMemory maxVal bytes => 0 (0 Bytes)\n",
      "Stage 89 JVMHeapMemory maxVal bytes => 972317960 (927,3 MB)\n",
      "Stage 89 OnHeapExecutionMemory maxVal bytes => 0 (0 Bytes)\n",
      "Stage 92 JVMHeapMemory maxVal bytes => 972317960 (927,3 MB)\n",
      "Stage 92 OnHeapExecutionMemory maxVal bytes => 0 (0 Bytes)\n"
     ]
    }
   ],
   "source": [
    "stagemetrics.print_memory_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
